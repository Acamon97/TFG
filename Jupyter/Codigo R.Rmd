---
title: "Pruebas R"
output: html_notebook
---


```{r}
# Instalamos los paquetes 
install.packages('rjson')
install.packages('tidyverse')
install.packages('quanteda')
install.packages('feather')
install.packages('cld2')
install.packages('ggplot2')
install.packages('plotly')
install.packages('scales')
install.packages('cowplot')
install.packages('tau')
install.packages('stopwords')
install.packages('tm')
install.packages('textstem')
install.packages('pbapply')
install.packages('text2vec')
install.packages('rlist')
install.packages('wordcloud')
install.packages('dbscan')
```




```{r}
# Cargamos los paquetes pertinenter
library(rjson); library(tidyverse); library(quanteda); library(feather); library(cld2); library(ggplot2); library(plotly); library(scales); library(cowplot); library(tau); library(stopwords); library(tm); library(textstem); library(pbapply); library(text2vec); library(rlist); library(wordcloud); library(dbscan)


```



CODIGO PARA CARGAR METADATA Y VER SU ESTRUCTURA

```{r}
# Cargamos los datos del archivo "metadata.csv"
# stringAsFactors evita que las cadenas de texto se conviertan en factores
# na.strings indica qué consideramos valores nulos. En este caso celdas vacías y "NA"
metadata_df <- read.csv("D:/COVID/metadata.csv", stringsAsFactors = FALSE, na.strings = c("", "NA"))
# str() permite ver la estructura del archivo: número de filas, columnas y muestra el primero objeto almacenado
str(metadata_df)

```


FUNCIÓN PARA EXTRAER INFORMACIÓN

```{r}
# Función de ayuda  para extraer información de JSON
leer_json <- function(json){
  # Obtenemos el identificador del texto
  paper_id <- json$paper_id
  # Obtenemos sus autores
  authors <- json$metadata$authors
  author_list <- list()
  # Separamos cada autor
  for (author in authors) {
      name <- paste0(author$first," ", author$last, ";")
      author_list <- paste0(author_list, as.character(name))
  }
  # Obtenemos el título del artículo
  title_text <- json$metadata$title
  # Obtenemos todo el abstract o resumen
  abstract_text <- ''
  for (each_abstract in json$abstract) {
      abstract_text <- paste(abstract_text, each_abstract$text)
  }
  # Obtenemos el cuerpo del artículo
  body_text <- ''
  for (each_body in json$body_text) {
      body_text <- paste(body_text, each_body$text)
  }

  # Devolvemos el resultado en forma de Data Frame
  return(tibble(paper_id, author_list, title_text, abstract_text, body_text, all_text))
}


```

DIRECTORIO DE LOS ARCHIVOS

```{r}

# Identificamos la carpeta que contiene los JSON
directorio <- 'D:/COVID/document_parses/pdf_json/'
#Cogemos todos los archivos del directorio
archivos <- list.files(directorio)
# Mostramos el número de archivos
length(archivos)

```

PROCESAMIENTO DE LOS ARHIVOS (covid_df.csv)

30 minutos

```{r}
# Inicializamos el contenido del artíuculo en formato lista
articulos <- list()
# Inicializamos el índice de artículos
indice <- 0

# Iteramos cada uno de los archivos
for (archivo in archivos) {
  # Actualizamos el puntero
  indice <- indice + 1
  # Breve comprobación para mostrar el estado de carga
  if (indice %% (length(archivos) %/% 10) == 0) {
    cat("Artículos procesados:  ", indice, " de ", length(archivos), "\n")
  }
  
  # Modificamos el camino o PATH hacia el archivo actual
  ruta_archivo <- paste0(directorio, '/', archivo)
  # Convertimos el archivo JSON a objeto de R para poder tratarlo
  json <- fromJSON(file = ruta_archivo)
  # Aplicamos la función para extraer la informaciópn del archivo
  contenido <- leer_json(json)
  
  # Comprobamos si el formato del archivo es el correcto
  if(length(contenido$paper_id) > 0){
    # Si se ha obtenido información del archivo
    # Buscamos el artículo en el archivo "metadata.csv"
    meta <- metadata_df[which(metadata_df$sha == contenido$paper_id), ] 
    # Si no hay información en metadata.csv o el cuerpo del artículo está vacío saltamos el artículo
    if (length(meta) > 0 | length(contenido$body_text) <= 0) {
      # Si no se encuentra autor en el JSON buscamos en metadata      
      if (length(contenido$author_list) <= 0) {
        authors <- metadata_df$authors
        author_list <- list()
        # Separamos cada autor
        for (author in authors) {
            name <- paste0(author$first," ", author$last, ";")
            author_list <- paste0(author_list, as.character(name))
        }
        # Sustituimos los autores
        contenido$author_list <- author_list
      }
      
      # Si no se encuentra el título en el JSON lo buscamos en metadata
      if (contenido$title_text == '') {
        contenido$title_text <- 'NoIncluido'
      }
      
      # Si no se encuentra el abstract lo dejamos como 'No incluido'
      if (nchar(contenido$abstract_text) <= 0) {
        contenido$abstract_text <- 'NoIncluido'
      }

      # Añadimos la revista de publicación
      if (length(meta$journal) > 0) {
        contenido$journal <- meta$journal[1]
      }
      else {
        contenido$journal <- 'NoIncluido'
      }
      # Añadimos el DOI
      if (length(meta$doi) > 0) {
        contenido$doi <- meta$doi[1]
      }
      else {
        contenido$doi <- 'NoIncluido'
      }
   
      # Incluimos el contenido al conjunto de artículos
      articulos[[indice]] <- contenido
    }
  }
}

# Enlazamos todas las filas generadas en un único Data Frame
covid_df <- bind_rows(articulos)

```

```{r}
# Observamos las cinco primeras filas del nuevo Data Frame
view(head(covid_df,5))
# Comprobamos el número de artículos existentes
nrow(covid_df)

```

6 minutos

```{r}
write.csv(covid_df, "D:/COVID/covid_df_prelematizacion.csv", row.names = FALSE)
```

```{r}
covid_df <- read.csv("D:/COVID/covid_df.csv", stringsAsFactors = FALSE, na.strings = c("", "NA"))
```

30 segundos
```{r}
write_feather(covid_df, "D:/COVID/covid_df_feather_postlematizacion.csv")

```

20 segundos
```{r}
covid_df <- read_feather("D:/COVID/covid_df_feather_pretokenizacion.csv")
```

(covid_df_feather_sin_todo.csv)

```{r}
covid_df <- select(covid_df, -all_text)
```


¡¡¡¡CONTADOR DE PALABRAS!!!! (/covid_df_feather_palabras)
1 minutos
```{r}
# Contamos el número de palabras para el abstracto ...
covid_df$words_abstract <- apply(covid_df['abstract_text'], 2, function(s) str_count(s, '\\w+'))
# ... y el cuerpo del texto
covid_df$words_body <- apply(covid_df['body_text'], 2, function(s) str_count(s, '\\w+'))

#covid_df['words_abstract'] <- apply(covid_df['abstract_text'], 2, function(s) str_count(s, '\\w+'))
#covid_df['words_body'] <- apply(covid_df['body_text'], 2, function(s) str_count(s, '\\w+'))

```



¡¡¡¡DUPLICADOS!!! (covid_df_feather_duplicados)
5 segundos
```{r}
# Comprobamos cuantos objetos tienen el mismo cuerpo
duplicados <- duplicated(covid_df$body_text)
# Mostramos la información obtenida
summary(duplicados)
# Obtenemos los 5 primeros objetos detectados como duplicados 
head(which(duplicados==TRUE))

```

```{r}
# Mostramos los cinco primeros duplicados
view(covid_df[c(2176,2590,4426,5000,5018),])

```

¡¡¡¡MOSTRAR DUPLICADOS!!!!

```{r}
length(which(covid_df$title_text=='Autoantibodies in Patients with Rheumatoid Arthritis'))
length(which(covid_df$title_text=='Remdesivir in Treatment of COVID-19: A Systematic Benefit-Risk Assessment'))
length(which(covid_df$title_text=='Extracellular superoxide dismutase, a molecular transducer of health benefits of exercise'))
length(which(covid_df$title_text=='COVID-19 pneumonia: different respiratory treatments for different phenotypes?'))

```

¡¡¡¡ELIMINAR DUPLICADOS!!!!

```{r}
# Eliminamos duplicados
covid_df <- covid_df[!duplicated(covid_df$body_text),]
nrow(covid_df)

```


¡¡¡¡DETECCION LENGUAJE!!!! (covid_df_feather_idioma.csv)

```{r}
# Añadimos una nueva columna con el idioma de cada texto
covid_df$language <- apply(covid_df['body_text'], 2, function(s) detect_language(substring(s, 1,  2000)))
# Información
summary(covid_df$language)


```

Mirar de hacerlo con ggplot

```{r}
# Creación de matriz con los datos
idiomas <- matrix(c(842, 92212, 658, 517, 130, 71, 88), ncol = 7, byrow = TRUE)
colnames(idiomas) <- (c('Alemán','Inglés', 'Español', 'Francés', 'Holandés', 'Otros', 'NA'))
# Generación de gráfico 
barplot(idiomas, main = 'Distribución de idiomas', xlab = 'Idiomas', ylab = 'Número de artículos', col = '#69b3a2', axes = FALSE)
usr <- par("usr")
par(usr=c(usr[1:2], 0, 100000))
axis(2, at=seq(0, 100000,25000))


```

¡¡¡¡FILTRADO DE INGLES!!!! (covid_df_feather_ingles.csv)

```{r}
# Nos quedamos únicamente con textos en inglés
covid_df <- covid_df[which(covid_df$language=='en'),]
```

¡¡¡¡INFORMACION PALABRAS!!!!
```{r}
# Información sobre el número de palabras
summary(covid_df[,c('words_abstract','words_body')])

```


```{r}
# Generación del grafo para el resumen con histograma y densidad
palabras_abstract <- ggplot(covid_df, aes(x=words_abstract)) + geom_histogram(aes(y=..density..), alpha=.3, fill="cyan", bins=80) + geom_density( colour="blue") + theme_classic() + ggtitle('Distribución del número de palabras en el resumen') +  xlab('Número de palabras del resumen') + ylab('Densidad')
# Misma operación para el cuerpo del texto
palabras_cuerpo <- ggplot(covid_df, aes(x=words_body)) + geom_histogram(aes(y=..density..), alpha=.3, fill="cyan", bins=80) + geom_density( colour="blue") + theme_classic() + scale_x_continuous(labels = comma) + ggtitle('Distribución del número de palabras en el cuerpo')  +  xlab('Número de palabras del cuerpo') + ylab('Densidad')
# Mostramos los gráficos
palabras_abstract
palabras_cuerpo

```

¡¡¡¡MAS INFORMACION PALABRAS!!!!

```{r}
# Cinco primeros artículos con menos de diez palabras en el abstracto
view(covid_df[head(which(covid_df$words_abstract < 10 & covid_df$abstract_text != 'NoIncluido')),])
# Y menos de diez palabras en el cuerpo
view(covid_df[head(which(covid_df$words_body < 10)),])

```


¡¡¡¡ELIMINACION DE PALABRAS!!!!


```{r}
# Cargamos la lista del paquete stopwords
p_vacias <- stopwords::stopwords("en",source = "stopwords-iso")
# Añadimos más palabrasa 
p_propias <- c('doi', 'preprint', 'copyright', 'org', 'https', 'et', 'al', 'author', 'figure', 'table',
    'rights', 'reserved', 'permission', 'use', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI',
    '-PRON-', 'usually')
# Y las juntamos
p_vacias <- append(p_vacias, p_propias)
# Eliminamos cualquier posible duplicado
p_vacias <- unique(p_vacias)
# Vemos información al respecto
length(p_vacias)
sample(p_vacias, 10)

```


3 minutos (covid_df_feather_abstract.csv)

```{r}
# Almacenamos el primer resumen para comparar
resumen <- covid_df$abstract_text[1]
# Conversión del resumen a minúsculas
covid_df$abstract_text <- apply(covid_df['abstract_text'], 2, function(s) tolower(s))
# Eliminación de signos de puntuación
covid_df$abstract_text <- apply(covid_df['abstract_text'], 2, function(s) removePunctuation(s))
# Eliminación de palabras vacías
covid_df$abstract_text <- apply(covid_df['abstract_text'], 2, function(s) remove_stopwords(s, p_vacias, lines = TRUE))
# Añadimos nueva columna con número de palabras actuales
covid_df$new_word_abstract <- apply(covid_df['abstract_text'], 2, function(s) str_count(s, '\\w+'))


#covid_df['abstract_text'] <- apply(covid_df['abstract_text'], 2, function(s) tolower(s))
#covid_df['abstract_text'] <- apply(covid_df['abstract_text'], 2, function(s) removePunctuation(s, preserve_intra_word_contractions = TRUE, preserve_intra_word_dashes = TRUE))
#covid_df['abstract_text'] <- apply(covid_df['abstract_text'], 2, function(s) remove_stopwords(s, p_vacias, lines = TRUE))
#covid_df['new_word_abstract'] <- apply(covid_df['abstract_text'], 2, function(s) str_count(s, '\\w+'))
```      


```{r}
substring(covid_df$body_text[1], 1, 2000)
```

10 minutos

```{r message=TRUE, warning=TRUE}
# Repetimos la operación con el cuerpo
# Conversión del cuerpo a minúsculas
covid_df$body_text <- apply(covid_df['body_text'], 2, function(s) tolower(s))
# Eliminación de signos de puntuación
covid_df$body_text <- apply(covid_df['body_text'], 2, function(s) removePunctuation(s))
```

3 horas (covid_df_feather_stopwords.csv)

```{r}
# Eliminación de palabras vacías
covid_df$body_text <- apply(covid_df['body_text'], 2, function(s) remove_stopwords(s, p_vacias, lines = TRUE))

```

```{r}
# Añadimos nueva columna con número de palabras actuales
covid_df$new_word_body <- apply(covid_df['body_text'], 2, function(s) str_count(s, '\\w+'))

```

¡¡¡¡COMPARAMOS NUMERO DE PALABRAS!!!!

```{r}
# Creamos dos nuevas columnas con la resta de palabras originales y actuales
covid_df$abstract_comparative <- covid_df$words_abstract - covid_df$new_word_abstract
covid_df$body_comparative <- covid_df$words_body - covid_df$new_word_body
```

```{r}
# Mostramos un resumen de la información
summary(covid_df[c('new_word_abstract', 'new_word_body', 'abstract_comparative', 'body_comparative')])
# Calculamos el total de palabras iniciales
inicial_resumen <- sum(covid_df$words_abstract)
inicial_cuerpo <- sum(covid_df$words_body)
inicial <- inicial_resumen + inicial_cuerpo
# Calculamos el total de palabras finales
final_resumen <- sum(covid_df$new_word_abstract)
final_cuerpo <- sum(covid_df$new_word_body)
final <- final_resumen + final_cuerpo
# Calculamos las palabras eliminadas
quitadas_resumen <- sum(covid_df$abstract_comparative)
quitadas_cuerpo <- sum(covid_df$body_comparative)
quitadas <- quitadas_resumen + quitadas_cuerpo
# Mostramos los datos
datos <- matrix(c(inicial_resumen, inicial_cuerpo, inicial, final_resumen, final_cuerpo, final, quitadas_resumen, quitadas_cuerpo, quitadas), nrow = 3, dimnames = list(c("Resumen", "Cuerpo", "Total"), c("Inicial", "Final", "Eliminadas")))
datos


```


¡¡¡¡GRAFICOS PARA LAS PALABRAS!!!!

```{r}
# Generación del grafo para el resumen con histograma y densidad
nuevas_palabras_abstract <- ggplot(covid_df, aes(x=new_word_abstract)) + geom_histogram(aes(y=..density..), alpha=.3, fill="cyan", bins=80) + geom_density( colour="blue") + theme_classic() + ggtitle('Distribución del número de palabras en el resumen') + xlab('Número actual de palabras del resumen') + ylab('Densidad')
# Misma operación para el cuerpo del texto
nuevas_palabras_cuerpo <- ggplot(covid_df, aes(x=new_word_body)) + geom_histogram(aes(y=..density..), alpha=.3, fill="cyan", bins=80) + geom_density( colour="blue") + theme_classic() + scale_x_continuous(labels = comma) + ggtitle('Distribución del número de palabras en el cuerpo') + xlab('Número actual de palabras del cuerpo') + ylab('Densidad')
# Generamos grafo para ver cuantas palabras se han quitado
comparacion_abstract<-ggplot(covid_df, aes(x=abstract_comparative)) + geom_histogram(aes(y=..density..), alpha=.3, fill="red", bins=80) + geom_density( colour="red") + theme_classic() + ggtitle('Palabras eliminadas del resumen') +  xlab('Número de palabras retiradas del resumen') + ylab('Densidad') + geom_vline(aes(xintercept= mean(abstract_comparative)), linetype="dashed")
# Misma operaciópn para el cuerpo del texto
comparacion_cuerpo <- ggplot(covid_df, aes(x=body_comparative)) + geom_histogram(aes(y=..density..), alpha=.3, fill="red", bins=80) + geom_density( colour="red") + theme_classic() + scale_x_continuous(labels = comma) + ggtitle('Palabras eliminadas del resumen') + xlab('Número de palabras retiradas del cuerpo') + ylab('Densidad') + geom_vline(aes(xintercept= mean(body_comparative)), linetype="dashed")

# Mostramos los gráficos
nuevas_palabras_abstract
nuevas_palabras_cuerpo
comparacion_abstract
comparacion_cuerpo
```




(covid_df_feather_pretokenizacion.csv)
¡¡¡¡TOKENIZACION!!!!

```{r}
# Tokenizamos tanto abstracto como texto completo
covid_df['abstract_text'] <- apply(covid_df['abstract_text'], 2, function(s) word_tokenizer(s, xptr = TRUE, pos_keep = character('-')))
covid_df['body_text'] <- apply(covid_df['body_text'], 2, function(s) word_tokenizer (s, xptr = TRUE))

```

```{r}
# Mostramos los primeros cinco términos de los resúmenes y el cuerpo
covid_df$abstract_text[[1]][1:5]
covid_df$body_text[[1]][1:5]

```

Resultado de la tokenización:
Dos mins
```{r}
# Guardar listas tokens
list.save(covid_df$abstract_text, file="D:/COVID/tokens_abs.RData")
list.save(covid_df$body_text, file="D:/COVID/tokens_body.RData")

```

Listas de tokens cargarlas en sus columnas

```{r}
# Cargar listas tokens
covid_df$abstract_text <- list.load("D:/COVID/tokens_abs.RData")
covid_df$body_text <- list.load("D:/COVID/tokens_body.RData")
```



¡¡¡¡LEMATIZACION!!!!
Cargar (covid_df_feather_pretokenizacion.csv) y meter las listas en las columnas
4 y media horas
```{r}
# Haciendo esto apply <- lapply conseguimos una paralelización del trabajo gracias a la lista que genera la tokenización de los textos
system.time(covid_df['abstract_text'] <- apply(covid_df['abstract_text'], 2, function(s) lapply(s, function(t) lemmatize_words(t))))
system.time(covid_df['body_text'] <- apply(covid_df['body_text'], 2, function(s) lapply(s, function(t) lemmatize_words(t))))

```

!!MOSTRAR LEMATIZACION!!

Algo para comparar y ver que en efecto, se han lematizado las palabras
```{r}
# Mostramos los primeros cinco términos de los resúmenes y el cuerpo
covid_df$abstract_text[[1]][1:5]
covid_df$body_text[[1]][1:5]
```

!!!Comparación palabras únicas!!
```{r}
iniciales <- covid_df$words_body[1:5]
# Calculamos el total de palabras del cuerpo de los cinco primeros artículos...
finales <- c(length(covid_df$body_text[[1]]), length(covid_df$body_text[[2]]), length(covid_df$body_text[[3]]), length(covid_df$body_text[[4]]), length(covid_df$body_text[[5]]))
#... y las palabras únicas de los mismos
unicas <- c(length(unique(covid_df$body_text[[1]])), length(unique(covid_df$body_text[[2]])), length(unique(covid_df$body_text[[3]])), length(unique(covid_df$body_text[[4]])), length(unique(covid_df$body_text[[5]])))
# Creamos una matriz para comparar los valores y damos nombre a filas y columnas
comparacion <- matrix(c(iniciales, finales, unicas), nrow = 3, byrow = T)
dimnames(comparacion)<-list(c("Iniciales", "Finales","Unicas"), c("Artículo 1","Artículo 2","Artículo 3","Artículo 4","Artículo 5"))
comparacion
```


Para guardar las listas de palabras lemmatizadas

Dos mins
```{r}
# Guardar listas tokens
list.save(covid_df$abstract_text, file="D:/COVID/lemas_abs.RData")
list.save(covid_df$body_text, file="D:/COVID/lemas_body.RData")

```

Listas de lemas cargarlas en sus columnas

```{r}
# Cargar listas lemas
covid_df$abstract_text <- list.load("D:/COVID/lemas_abs.RData")
covid_df$body_text <- list.load("D:/COVID/lemas_body.RData")
```


¡MATRIZ DTM!

```{r}
covid <- head(covid_df, 1000)
```

```{r}
it <- itoken(covid$body_text, ids = covid$paper_id)
vocab <- create_vocabulary(it)
vocab <- prune_vocabulary(vocab, vocab_term_max = 2^12)
vect <- vocab_vectorizer(vocab) 
dtm <- create_dtm(it, vect)

tfidf = TfIdf$new()
tfidf <- as.matrix(fit_transform(dtm, tfidf))

#tfidf = sim2(x = dtm, method = "cosine", norm = "l2")

#tfidf = as.DocumentTermMatrix(dtm, weighting = weightTfIdf)


```


```{r}
tst = round(ncol(tfidf)/1000)
a = rep(tst, 999)
b = cumsum(a);rm(a)
b = c(0,b,ncol(tfidf))

ss.col = c(NULL)
for (i in 1:(length(b)-1)) {
  tempdtm = tfidf[,(b[i]+1):(b[i+1])]
  s = colSums(as.matrix(tempdtm))
  ss.col = c(ss.col,s)
}

tsum = ss.col

tsum = tsum[order(tsum, decreasing = T)]       #terms in decreasing order of freq
head(tsum)


tail(tsum)
```

```{r}
wordcloud(names(tsum), tsum, scale=c(4,0.5),1, max.words=50,colors=brewer.pal(8, "Dark2")) # Plot results in a word cloud 
title(sub = "Term Frequency Inverse Document Frequency - Wordcloud")
```


```{r}
dbscan_1 <- dbscan(tfidf, minPts = 5, eps = 5)
summary(dbscan_1)
```


```{r}
plot(dbscan_1$cluster, tfidf)

```








