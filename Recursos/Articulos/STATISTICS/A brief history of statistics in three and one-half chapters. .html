<!DOCTYPE html>
<html id="ctl00__htmlTag" class=" js canvas csscolumns video audio" lang="es"><head><title>
	EBSCOhost
</title><meta http-equiv="content-type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><link rel="icon" href="https://if.ebsco-content.com/interfacefiles/17.299.0.2936/ehost/favicon.ico" type="image/x-icon">
<link rel="shortcut icon" href="https://if.ebsco-content.com/interfacefiles/17.299.0.2936/ehost/favicon.ico" type="image/x-icon">

		<link rel="stylesheet" type="text/css" href="EBSCOhost_files/master_bundle.css" media="All">
<link rel="stylesheet" type="text/css" href="EBSCOhost_files/abody.css" media="All">
<link rel="stylesheet" type="text/css" href="EBSCOhost_files/fontawesome_min.css" media="All">
<link rel="stylesheet" type="text/css" href="EBSCOhost_files/delivery.css" media="All">
<link rel="stylesheet" type="text/css" href="EBSCOhost_files/inject.css" media="All">
<link rel="stylesheet" type="text/css" href="EBSCOhost_files/print.css" media="Print">
<!--[if lt IE 9]><link rel="stylesheet" type="text/css" href="https://if.ebsco-content.com/interfacefiles/17.299.0.2936/css/_layout2/ie8.css" media="All" /><![endif]-->
<!--[if lt IE 8]><link rel="stylesheet" type="text/css" href="https://if.ebsco-content.com/interfacefiles/17.299.0.2936/css/_layout2/ie7.css" media="All" /><![endif]-->
<!--[if lt IE 7]><link rel="stylesheet" type="text/css" href="https://if.ebsco-content.com/interfacefiles/17.299.0.2936/css/_layout2/ie6.css" media="All" /><![endif]-->
<!--##EPCSS##-->
	</head>
	<body id="ctl00__bodyTag" class="no-skin delivery ehost">
		
		<div id="epAjaxActive">Cargando...</div>
		<form method="post" action="./delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d" id="aspnetForm">
<div class="aspNetHidden">
<input type="hidden" name="RelRequestPath" id="RelRequestPath" value="delivery">
<input type="hidden" name="__EVENTTARGET" id="__EVENTTARGET" value="">
<input type="hidden" name="__EVENTARGUMENT" id="__EVENTARGUMENT" value="">
<input type="hidden" name="__sid" id="__sid" value="dc44e27c-921a-49db-8a7f-54d11c0e1489@pdc-v-sessmgr06">
<input type="hidden" name="__vid" id="__vid" value="26">
<input type="hidden" name="__CUSTOMVIEWSTATE" id="__CUSTOMVIEWSTATE" value="H4sIAAAAAAAEAC2MwQrCMBBEyZpU0EMOQo7aH+ihUIM/4UWk9+iuJhgadKPo12uqHt/MvHkLjVrNm7az603XWdtqA8tF72JAl2lH1ztx3iYkEAZOGg2AGCcyuuGsgLgEkzFQx+iYlRlSw5cw1EgxPOj2qsknzmWlvnL1v0BT/Ty5p2dWosVSFZz2gcMhkkeYFV5Jjx/4B+5QpAAAAA==">
<input type="hidden" name="__VIEWSTATE" id="__VIEWSTATE" value="">
</div>

<script type="text/javascript">
//<![CDATA[
var theForm = document.forms['aspnetForm'];
if (!theForm) {
    theForm = document.aspnetForm;
}
function __doPostBack(eventTarget, eventArgument) {
    if (!theForm.onsubmit || (theForm.onsubmit() != false)) {
        theForm.__EVENTTARGET.value = eventTarget;
        theForm.__EVENTARGUMENT.value = eventArgument;
        theForm.submit();
    }
}
//]]>
</script>



<script type="text/javascript">
//<![CDATA[
var isDeliverySaveHtml=true;//]]>
</script>

		<script>
var ep = {"version":"17.299.0.2936","baseImagePath":"https://if.ebsco-content.com/interfacefiles/17.299.0.2936/","brandingPath":"https://imageserver.ebscohost.com/branding/","interfaceId":"ehost","cssLayout":2,"messages":{"Close":"Cerrar","Loading":"Cargando","show_this_area":"Mostrar esta área","hide_this_area":"Ocultar esta área"},"clientData":{"pid":"s3205084.main.ehost","lid":"http://search.ebscohost.com/?authtype=cookie,ip,uid","esid":"dGJyMOLZskzjqLV7q6+wSd+jslHi2KtQ363kRbOq4kmv2a59r6q2Ub7m4nur7KuL4+nxheXork4A","db":["aph"],"deviceId":"5d3ba30c-9f67-4c19-88cb-ea41ee584180","interface_version":"live","auth_type":"ip","is_personalized":false,"user_id":"00000000-0000-0000-0000-000000000000","is_amplitude_error_logging_enabled":false},"templates":{},"pageScripts":["bundled/jqueryplusui.js","bundled/underscore.js","bundled/_layout2/master.js","ep/authentication-aspect.js","ep/cssr.js","ep/autoevent.js"],"relativeRequestPath":"delivery","sid":"dc44e27c-921a-49db-8a7f-54d11c0e1489@pdc-v-sessmgr06","vid":"26","existingReturnUrl":"https://web.b.ebscohost.com/ehost/command/detail?vid=25&sid=dc44e27c-921a-49db-8a7f-54d11c0e1489@pdc-v-sessmgr06&bdata=Jmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU=","newReturnUrl":"/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489@pdc-v-sessmgr06&vid=26&ReturnUrl=https://web.b.ebscohost.com/ehost/command/detail?vid=25&sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&bdata=Jmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%3d","locale":"es","isEnabledDatabaseHyperlink":false,"isEnabledDatabaseHyperlinkForProfile":false,"amplitudeAPIKey":"408774472b1245a7df5814f20e7484d0","palTestCaseId":""}
</script>
<script src="EBSCOhost_files/ep_boot.js"></script>
<!--[if lt IE 9]>
<script src="https://if.ebsco-content.com/interfacefiles/17.299.0.2936/javascript/html5shiv/html5.js"></script>
<![endif]-->
<script>
if (typeof isDeliverySaveHtml === 'undefined'){
ep.boot(null,null);}
</script>
<!--##EPJS##-->
			<!--[if lt IE 7]>	
			
			<![endif]-->

			
			
			
			<a class="accessibility-support-link" href="javascript:openWideTip('http://support.ebsco.com/help/?int=ehost&amp;lang=es&amp;feature_id=access&amp;TOC_ID=Always&amp;SI=0&amp;BU=0&amp;GU=1&amp;PS=0&amp;ver=&amp;dbs=aph')">Información y consejos sobre accesibilidad</a> <span class="hidden">Fecha revisada: 07/2015</span>
			
			
			
	

<div id="deliveryOutput">
	<div class="backButton">
		<a id="ctl00_MainContentArea_deliveryPrintSaveControl_backButton_lnkBack" title="Atrás" class="arrow-link prev delivmngr-toolbar-link" href="javascript:__doPostBack('ctl00$MainContentArea$deliveryPrintSaveControl$backButton$lnkBack','')">Atrás</a>
	</div>
	<div id="estimate">
		1 artículo(s) se guardarán. 
	</div>
	<div id="notes">
		
		<div id="ctl00_MainContentArea_deliveryPrintSaveControl_saveNote">
		<p>
			<span>Para continuar, en Internet Explorer, seleccione</span>
			<strong> ARCHIVO</strong>
			<span> a continuación</span>
			<strong> GUARDAR COMO</strong>
			<span> en la barra de herramientas del navegador que verá más arriba.</span>
			<span> Asegúrese de guardar en formato de archivo de texto (.txt) o de página Web (.html).</span>
			<span>En Firefox, seleccione</span>
			<strong> ARCHIVO</strong>
			<span> a continuación</span>
			<strong> GUARDAR ARCHIVO COMO</strong>
			<span> en la barra de herramientas del navegador que verá más arriba.</span>
			<span>En Chrome, haga clic con el botón secundario (del mouse) en esta página y seleccione</span>
			<strong> GUARDAR COMO</strong>
		</p>
		</div>
	</div>
	
	<div id="records">
		<br><br><br><span class="medium-bold">
			EBSCO Publishing&nbsp;&nbsp;&nbsp;Formato de citas:
			APA (American Psychological Assoc.):
				</span><br><br><span class="citation-format-instructions"><em><strong>NOTA:</strong></em> repase las instrucciones en <a data-auto="ep_link" href="javascript:openWideTip('http://support.ebsco.com/help/?int=ehost&amp;lang=&amp;feature_id=APA');" id="need_help" title="http://support.ebsco.com/help/?int=ehost&amp;lang=&amp;feature_id=APA
							">http://support.ebsco.com/help/?int=ehost&amp;lang=&amp;feature_id=APA
							</a> y realice las correcciones necesarias antes de implementar este formato. <strong>Preste especial atención a los nombres propios, las fechas y el uso de las mayúsculas.</strong> Siempre consulte los recursos de la biblioteca en cuanto a normas de formato y puntuación. <br><br></span><hr><span class="medium-bold">Referencias</span><span class="medium-normal"><p style="margin-left:.5in;text-indent:-.5in">Fienberg, S. E. (1991). A brief history of statistics in three and one-half chapters. <i>Historical Methods</i>, <i>24</i>(3), 124. https://doi.org/10.1080/01615440.1991.9955300</p>&lt;!--Información adicional:<br>Vínculo persistente a este informe (enlace permanente): <a href="https://search.ebscohost.com/login.aspx?direct=true&amp;db=aph&amp;AN=9709120309&amp;lang=es&amp;site=ehost-live&amp;scope=site">https://search.ebscohost.com/login.aspx?direct=true&amp;db=aph&amp;AN=9709120309&amp;lang=es&amp;site=ehost-live&amp;scope=site</a><br>Fin de la cita--&gt;<p></p><hr><div class="print-ft-content"><br><b class="center">A BRIEF HISTORY OF STATISTICS
IN THREE AND ONE-HALF CHAPTERS&nbsp;</b><br><a id="AN9709120309-2"> </a><p class="medium-bold" data-auto="item_title">A Review Essay</p><pre class="ct">LORRAINE J. DASTON.
Classical Probability in the Enlightenment.
Princeton: Princeton University Press, 1988.

GERD GIGERENZER, ZENO SWIJTINK, THEODORE
PORTER, LORRAINE DASTON, JOHN BEATTY, and
LORENZ KRUGER.
The Empire of Chance. How Probability Changed
Science and Everyday Life. Cambridge:
Cambridge University Press, 1989.

ANDERS HALD.
A History of Probability and Statistics
and Their Applications Before 1750
New York: Wiley, 1990.

LORENZ KRUGER, LORRAINE DASTON, and
MICHAEL HEIDELBERGER, eds.
The Probabilistic Revolution, Volume 1: Ideas in History.
Cambridge: MIT Press, 1987.

LORENZ KRUGER, GERD GIGERENZER, and MARY S.
MORGAN, eds.
The Probabilistic Revolution, Volume 2: Ideas in the
Sciences.
Cambridge: MIT Press, 1987.

THEODORE M. PORTER.
The Rise in Statistical Thinking, 1820-1900.
Princeton: Princeton University Press, 1986.

STEPHEN M. STIGLER.
The History of Statistics: The Measurement of Uncertainty
Before 1900.
Cambridge, Mass.: Harvard University Press, 1986. Reissued in
a paperback edition (1990).</pre><p class="body-paragraph" data-auto="body_paragraph">Writing
 a history of some aspect of science is often a daunting task, requiring
 the painstaking reexamination of source materials long forgotten by 
contemporary scientists and the blending of a knowledge of the substance
 of the science in question with a broader historical sense. 
Furthermore, unlike those working in other areas of history, the 
historian of science is constantly encountering examples of lapses in 
the scientific etiquette and scholarship of those subjects under study,(<a id="bib2up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib2" id="ref_linkbib2" title="n2">n2</a>)
 as well as instances of Stigler's Law of Eponymy, which has a less 
modest origin than the name might suggest and states in its simplest 
form that "no scientific discovery is named after its original inventor"
 (see Stigler 1980). Although this essay takes its title in part as a 
play on the titles of two recent books, one a highly popular account of 
the history of physics and astronomy as they relate to the beginning of 
the universe and the other a somewhat less popular work of fictional 
history,(<a id="bib3up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib3" id="ref_linkbib3" title="n3">n3</a>)
 I intend to provide a brief but accurate overview of selective aspects 
of the development of the field of statistics, largely drawing on a 
septet of recently published books as well as a personal assessment of 
the books themselves. To make my perspective clear from the outset, I 
note that I write not as a historian of science but as a statistician 
with a strong interest in the history of his own discipline. </p><p class="body-paragraph" data-auto="body_paragraph">A
 history of a scientific field, such as statistics, plays a special role
 for the field itself, helping statisticians understand some of the 
origins of their work as well as giving them a sense of what statistical
 discovery is all about. Because statistical thinking infuses so many 
other scientific fields today, the history of statistics more broadly 
plays an important backdrop to the history of science. Thus, one might 
well begin by asking, Who has actuary been writing about the history of 
statistics? In fact, only two of the many authors of the books under 
review are statisticians, Anders Hald and Stephen M. Stigler. The other 
authors and editors include economist Mary S. Morgan; historians John 
Beatty, Lorraine J. Daston, and Theodore M. Porter; philosophers of 
science Lorenz Kruger, Michael Heidelberger, and Zeno Swijtink; and 
psychologist Gerd Gigerenzer. The non-statisticians have knowledge about
 statistics, and they may bring to their historical inquiries a 
methodology that the statistician does not possess. For me, this raises 
the questions: What kind of training and knowledge should the historian 
of science have? How much substantive knowledge is necessary? The 
insight of statisticians into their own field may provide considerable 
power in the development of a history of the field, but such insights 
alone do not make for good historians of science. I raise these 
questions at the outset so that the reader may keep them in mind as I 
explore the historical developments chronicled in the books under 
review.(<a id="bib4up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib4" id="ref_linkbib4" title="n4">n4</a>)
 In the epilogue, I address them in a summary fashion by explaining what
 constitutes for me the basis for a good history of statistics. </p><p class="body-paragraph" data-auto="body_paragraph">The
 modern textbook description of statistics is the science of collecting,
 organizing, interpreting, and reporting data, where the data consist of
 observations taken in the real world. Most statistical works are cast 
in the language of probability, and statistical inference uses 
probability to express uncertainty. Yet, for me, as a statistician, 
probability theory is a branch of mathematics, whereas statistics is an 
area of science separate from mathematics. Because of the special links 
between probability and statistics, any history of statistics must deal 
with contributions and developments in the domain of probability. In 
this historical overview of the field of statistics provided in the 
present essay, I include developments within the domain of probability 
but primarily when the probabilistic developments relate to specific 
statistical ones. </p><p class="body-paragraph" data-auto="body_paragraph">The
 history that follows has four identifiable periods: 1660-1750, 
1750-1820, 1821) 1900, and 1900-1950. In part, the choice of periods is 
linked to the ideas and exposition in several of the books under review.
 In particular, the end of each of the first three periods marks the 
completion of a major probabilistic or statistical development as 
chronicled by one or more of the authors. Chapter I deals with what is 
best referred to as prehistory when many of the basic ideas in classical
 probability theory were developed but when there was little in the way 
of development of statistical ideas. Chapter 2 takes up the basic 
development of formal statistical methods, primarily linked to problems 
in the physical sciences, especially astronomy, by such eminent 
scientists as Karl Friedrich Gauss, Pierre Simon Laplace, and Adrien 
Marie Legendre, as well as the Reverend Thomas Bayes. The rise of 
statistical theory linked to the social and biological sciences is the 
focus of chapter 3, which covers the period from 1820 through 1900. 
Stigler argues that "the infant discipline of [statistics] may be said 
to have arrived" in 1900. Modern statistical methodology and theory as 
we know it is a twentieth-century creation, even though it is rooted in 
these earlier developments. Nonetheless, my overview of the history of 
statistics devotes only one-half of a chapter to this modern period, 
which ends sometime around World War II, mainly because the 
accomplishments of the past four decades are still too fresh for us to 
sort out which ones will ultimately be viewed as lasting by those in 
future generations.(<a id="bib5up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib5" id="ref_linkbib5" title="n5">n5</a>)
 To close the history, I offer an epilogue that considers some 
overarching issues and returns explicitly to the books on which I draw 
throughout the article. </p><p class="body-paragraph" data-auto="body_paragraph">The
 seven books under review deal with overlapping periods and overlapping 
material, often leaving the reader with quite different impressions of 
the relative importance of some contributions to the development of the 
field. Because the writing of such histories of science involves 
considerable subjectivity, these differences in perspective make each 
separate volume of interest, and thus the seven books collectively offer
 a better picture than a single authoritative volume. Each book has its 
virtues (and often its flaws), and, in the epilogue, I will supply my 
assessment of those under review. Nonetheless, I note at the outset that
 all are valuable additions to my personal bookshelf and belong in any 
major library. </p><p class="body-paragraph" data-auto="body_paragraph">Before
 turning to my abbreviated history, let me give some indication of the 
coverage of each of the seven books from the perspective of my three and
 one-half chapters rather than simply from a description of the contents
 of individual volumes. Hald describes the prehistorical period through 
1750, as does half of the first chapter of The Empire of Chance. In 
Classical Probability in the Enlightenment, Daston covers both the 
prehistorical period and the next century but mainly with a focus on the
 development of probability and its interpretation. She stresses the 
broader historical and philosophical perspectives, whereas Hald focuses 
on the narrower technical one. Stigler also describes in detail the key 
contributions of Jakob Bernoulli and Abraham De Moivre, and, in some 
ways, the work of these two figures represents the boundary between 
prehistory and history. Stigler's book is my primary source for the 
period from 1750 to 1820, although there is also the second half of the 
opening chapter of The Empire of Chance, parts of Classical Probability 
in the Enlightenment, and some individual chapters in volume 1 of The 
Probabilistic Revolution. Porter and Stigler focus on the period from 
1820 to 1900, and there are also related discussions in the second 
chapter of The Empire of Chance and selected material from volumes 1 and
 2 of The Probabilistic Revolution. Developments in the twentieth 
century are the primary focus of The Empire of Chance, as well as of 
several chapters from volumes 1 and 2 of The Probabilistic Revolution. </p><p class="body-paragraph" data-auto="body_paragraph">Two
 somewhat older histories are worth mentioning here as well, both 
covering approximately the same period: the mid-nineteenth-century 
history by Isaac Todhunter (1865) and the early-twentieth-century 
lectures by Karl Pearson (1978), which were posthumously published 
forty-two years after his death. Todhunter was long viewed as the 
unquestioned authority for the contributions from the periods covered by
 my first two chapters, but, with the current revival of interest in the
 history of probability and statistics, most knowledgeable readers 
recognize that Todhunter lacked a broad statistical perspective, and he 
often "meticulously report[ed] proofs of many results which are of 
little interest today; conversely, he omit[ted] proofs of results of 
great importance" (p. 9 in Hald's book). Furthermore, Todhunter did not 
really set contributions into their broader scientific context. Pearson,
 on the other hand, reveled in the contextual material and often 
digressed into the lives of the statisticians and mathematicians whose 
work he was describing. Unlike Todhunter, who is known primarily for his
 historical volumes, Pearson was a distinguished statistician, and his 
views of the historical contributions of others are seen through the 
filter of his own work and ideas (which were many). His volume does 
contain errors of fact, but most are corrected in the volumes under 
review here. </p><a id="AN9709120309-4"> </a><span class="medium-bold"><h3><a data-auto="ep_link" href="#toc" onclick="FocusElement('toc');" id="hd_toc_AN9709120309-4" title=" CHAPTER 1: PREHISTORY THROUGH 1750  "> CHAPTER 1: PREHISTORY THROUGH 1750  </a></h3></span><p class="body-paragraph" data-auto="body_paragraph">Where
 shall the history of statistics begin? This is the title of a brief 
1960 article by the statistician Maurice Kendall, who argues that "there
 are dangers in pursuing the roots of a subject down to its slenderest 
fibrils." He concludes "that statistics in any sense akin to our own 
cannot be traced back before about A.D. 1660" and points to the work of 
John Graunt as an appropriate starting point. Stigler also cautions us 
not to begin the history too early: "If all sciences require 
measurement--and statistics is the logic of measurement--it follows that
 the history of statistics can encompass the history of all science." He
 suggests that it is reasonable to restrict the history to the: 
development of probability-based statistical methods. In this spirit, we
 label the period of the development of probability and the exposition 
of non-probabilistic methods of data analysis as prehistory, and our 
history proper begins around 1750. (see chapter 2) </p><p class="body-paragraph" data-auto="body_paragraph">Both
 Daston and Hald note, as have many others before, that probability 
theory was originally inspired in large part by games of chance, and 
they recognize the formative role of the Italian mathematician Girolamo 
Cardano in developing probabilistic ideas in the sixteenth century. Hald
 then distinguishes three periods from 1660 through 1750 when crucial 
developments in the history of probability occurred. </p><p class="body-paragraph" data-auto="body_paragraph">From
 1654 to 1665, we have the correspondence of Blaise Pascal and Pierre de
 Fermat and the development of results on the binomial, including 
binomial coefficients and Pascal's triangle. This period represents what
 Ian Hacking has referred to as "The Emergence of Probability."(<a id="bib6up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib6" id="ref_linkbib6" title="n6">n6</a>) </p><p class="body-paragraph" data-auto="body_paragraph">Then
 from 1708 to 1718, after about a fifty-year of stagnation, we see a 
flurry of activity from Pierre Remond de Montmort, the Bernoullis, and 
De Moivre, during which the elementary results were molded into a 
coherent theory of probability. Of special importance statistics during 
this period was the work of Jakob known as James or Jacques) Bernoulli, 
published posthumously in 1713, in which he not only provides a version 
of the law of large numbers but also discusses the concept of 
probability, introducing, perhaps for the first time, the subjective 
notion that probability is personal and with an individual's knowledge. 
However, Stigler points, out that Bernoulli did more than prove the law 
of large numbers. He also attempted to show how to quantify the number 
of observations required for an observed proportion to fall within a 
given amount of the true proportion with "moral certainty," that is, 
with a chance exceeding 1000/1001 (this being the illustrative example 
that ends Bernoulli's book). </p><p class="body-paragraph" data-auto="body_paragraph">In
 the final period of probabilistic prehistory, from 1718 to 1738, we see
 consolidation and extension. This was the period when De Moivre 
developed the normal approximation to the binomial (earlier, in 1712, he
 had derived what we now refer to as the Poisson approximation to the 
binomial more than a century before Simeon-Denis Poisson's own work on 
the topic), although, as Stigler notes, De Moivre thought of the normal 
curve more as a calculating device than as a continuous probability 
distribution in its own right. These results, along with the method of 
generating functions, were published in the second edition of his book, 
The Doctrine of Chances (De Moivre 1738). De Moivre's derivation of the 
normal approximation was in many ways his attempt to improve on 
Bernoulli's bound for the sample size required to achieve moral 
certainty, and he recognized the importance of square root of n, where n
 is the number of trials, as giving the scale on which deviations from 
the center of the distribution should be judged. Yet De Moivre stopped 
short of going beyond the binomial to a broader form of central limit 
theorem as we know it today, and he also stopped short of using the 
approximation to make inverse inferences about the binomial parameter p.
 </p><p class="body-paragraph" data-auto="body_paragraph">While De 
Moivre's work was widely circulated and brought to a successful 
completion the development of what we now call classical probability 
theory, the period up to 1750 did not produce a theory of statistics 
involving the application of probabilistic ideas to data Rather, the 
contributions to statistics prior to 1750 consisted mainly of examples 
of data analysis, often without the use of any explicit probabilistic 
ideas or the notion of uncertainty. The prototypic model of descriptive 
statistical analysis is that found in John Graunt's 1662 work on the 
Bills of Mortality (Graunt 1662). In it, he gave birth to a number of 
data analytical approaches that included (1) an examination of the 
trustworthiness of the data in the "bills" published over a sixty-year 
period; (<a id="bib2up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib2" id="ref_linkbib2" title="2">2</a>)
 a careful description of the mortality due to the plague, including a 
wonderful illustration of "imputation" of omitted deaths in certain 
years; (<a id="bib3up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib3" id="ref_linkbib3" title="3">3</a>)
 a detailed description and analysis of the sex ratio (the ratio of the 
number of males to the number of females) of births and deaths in London
 and Romsey, a country parish m Hampshire--there appears to be an 
amazing stability in the ratios; and (<a id="bib4up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib4" id="ref_linkbib4" title="4">4</a>)
 the development of what is at least in part can empirically based life 
table in order to answer questions about how many men of fighting age 
there were in London. Hald (p. 86) observes that Graunt's "statistical 
methods are seldom pronounced directly but are to be |found in his 
examples." </p><p class="body-paragraph" data-auto="body_paragraph">Graunt's
 first edition was only eighty-five pages in I length, but it stimulated
 work by a substantial number of others on these and other topics, and 
many of these individuals introduced the use of probabilistic tools into
 the analyses. For example, whereas Graunt merely speculated on the 
reasons for this stability of sex ratios, some fifty years later, John 
Arbuthnot actually attempted to test the hypothesis that the ratio is 1,
 using a binomial model and what is in effect a simple sign test (see p.
 278 in Hald's book). This work was then followed up by Nicholas 
Bernoulli, who explored further the appropriateness of the binomial 
model for this problem. Similarly, the Dutch mathematician and physicist
 Christian Huygens, in correspondence with his brother, followed up on 
Graunt's life table in 1669, giving it a probabilistic interpretation 
through the use of odds, and he calculated both the expected and median 
lifetime, noting how the two concepts differed. (See chapter 8 in Hald's
 book.) </p><p class="body-paragraph" data-auto="body_paragraph">Although
 the origin of classical probability theory was closely linked to 
gambling, it is important to recognize that many of the actors described
 m this chapter were deeply religious men (e.g., Blaise Pascal), whose 
works on probabilistic problems were driven largely by their theological
 concerns about proof to support the existence of God or the role of God
 in certain problems, such as what we now refer to as Pascal's wager 
concerning the existence of God. (For further details, see the 
discussion in Daston's book on pp. 60-63 and in Hald's book on p. 64). 
There is also Jakob Bernoulli's discussion of moral certainty in Ars 
Conjectandi and Arbuthnot's 1712 statistical argument for divine 
providence. Virtually all of these authors believed that the world was 
deterministic--God leaves nothing to chance--yet they focused on 
probability to describe both games of chance and issues of morality and 
theology. Daston (chapters 2 and 63 pursues this moral--theological 
theme and the link to expectations, and she includes a detailed 
discussion of the seminal work of Daniel Bernoulli (Jakob's nephew), 
who, in a 1738 paper, introduced the general idea of the modern notion 
of utility.(<a id="bib7up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib7" id="ref_linkbib7" title="n7">n7</a>) </p><a id="AN9709120309-5"> </a><span class="medium-bold"><h3><a data-auto="ep_link" href="#toc" onclick="FocusElement('toc');" id="hd_toc_AN9709120309-5" title=" CHAPTER 2: THE INTRODUCTION OF INFERENCE AND THE BEGINNING OF MATHEMATICAL STATISTICS: 1750-1820  "> CHAPTER 2: THE INTRODUCTION OF INFERENCE AND THE BEGINNING OF MATHEMATICAL STATISTICS: 1750-1820  </a></h3></span><p class="body-paragraph" data-auto="body_paragraph">There
 are two intertwined strands of statistical activity in the period 
beginning at about 1750 that were brought together at about 1820 in what
 Stigler refers to as the Gauss-Laplace synthesis and thereby set the 
foundations for what we have come to know as mathematical statistics. 
The first strand involves the development of the method of least squares
 for the estimation of unknown coefficients in linear equations (they 
were not quite the linear models we now speak about); the second strand 
deals with the development of probability-based inferences, growing out 
of the work of Bernoulli and De Moivre. We pick up the story of the 
development of inference first. </p><p class="body-paragraph" data-auto="body_paragraph">As
 we left off our history with De Moivre, he had written about the normal
 approximation to the binomial distribution and had stopped just short 
of using this approximation to make inferences about the binomial 
parameter p. Chronologically, the developments over the next fifty years
 begin with two papers by Englishmen: a 1755 paper by Thomas Simpson, 
which was followed quickly by a posthumously published 1764 paper by the
 Reverend Thomas Bayes. In this now-celebrated essay, Bayes developed 
and then utilized the inverse probability argument that later became 
associated with his name, Bayes's Theorem. He used the inverse 
probability approach to answer the problem that had stumped both 
Bernoulli and De Moivre--that is, he showed how to reason 
probabilistically about the binomial parameter p in light of the data. 
Then came an amazing series of results by the French mathematician and 
astronomer Pierre Simon Laplace in which he first approached the 
binomial problem using an inverse probability argument similar to that 
of Bayes and then went on to expand its use for a variety of other 
distributions. All of this ultimately led Laplace in 1810 to a clear 
formulation of the central limit theorem that could justify the use of 
the normal distribution to approximate the distribution of sums (or 
averages) from virtually any probability distribution. </p><p class="body-paragraph" data-auto="body_paragraph">Stigler
 presents these accomplishments in statistics proper out of 
chronological order. After describing Simpson's advances and Bayes's 
recognition of them, he skips ahead to Laplace, foreshadowing the story 
that is to come with the statement: </p><p class="body-paragraph" data-auto="body_paragraph">Simpson
 had seen that the concept of error distributions permitted a back-door 
access to the measurement of uncertainty. Later Laplace was to slip in 
this same back door and come around to open the front (only to find that
 Bayes's key was already in the lock). (p. 95) </p><p class="body-paragraph" data-auto="body_paragraph">Stigler
 painstakingly analyzes Laplace's (re)discovery of Bayes's Theorem and 
notes the slow progress he made at the beginning. But once he understood
 the power of the inverse probability approach, Laplace was able to move
 ahead rapidly to solve the problems that had thwarted Bayes. Thus, the 
contribution of Bayes, amazing though it was, appears to have had little
 direct influence on the development of the theory of mathematical 
statistics in this crucial period. </p><p class="body-paragraph" data-auto="body_paragraph">The
 other key strand of developments during this period relates to the 
evolution of a general statistical approach for combining observations 
that culminated in the method of least squares. Behind virtually all of 
these developments was a practical series of data problems in astronomy 
involving observations on planetary positions, orbits, and geodesic 
arcs. At the beginning of these developments, we have the 1750 study of 
Johann Tobias Mayer on the librations of the moon, m which he proposed 
an ingenious method for solving twenty-seven equations in three unknowns
 through the creation of three equations formed by summing the nine 
equations in each of three carefully constructed groups. Stigler 
contrasts Mayer's accomplishment with the 1749 failure of the 
distinguished mathematician Leonhard Euler to solve essentially the same
 problem--his explanation is Mayer's empirically based "conviction that a
 combination of observations increased the accuracy of the result in 
proportion to the number of equations combined" (p. 28). With an 
intervening contribution by Ruggiero Giuseppe Boscovich, who introduced 
the notion of a principle for combining, we come again to Laplace, who, 
in 1787, extended Mayer's approach to reducing a set of linear equations
 by combining them together in several different ways. The problem was 
that Laplace's approach was still ad hoc; that is, it did not involve an
 explicit mathematical criterion relating to the statistical aspects of 
the situation. Thus, different people trying to emulate Laplace's 
approach on a new set of equations could well produce different answers.
 Legendre resolved all of this with his 1805 development of the method 
of least squares, which invoked a statistical minimization principle and
 created a unique set of reduced equations that could then provide the 
estimates of the unknown coefficients based on the combined 
observations. </p><p class="body-paragraph" data-auto="body_paragraph">But
 how does the method of least squares fit with the quantification of 
uncertainty that had been developed in the other strand of research? 
This crucial link was provided by the great German mathematician Carl 
Friedrich Gauss, who, in 1809, used a circular argument to justify the 
use of normally distributed error terms for systems of linear equations 
and then went on to show that maximizing the posterior distribution of 
the errors was equivalent to using the method of least squares. The 
paper had an immediate impact on Laplace, who, in 1810, recognized that 
the normal error term could be justified by his own results on the 
central limit theorem. This fusing of the two lines of development into a
 single, powerful statistical approach applicable to a broad class of 
physical problems is what Stigler refers to as the Gauss-Laplace 
synthesis. It brings to a close our second chapter and leaves us with a 
question asked by Stigler in his introduction: If the Gauss-Laplace 
synthesis essentially produced the methodology of regression analysis, 
why did another seventy-five years go by before Galton invented 
regression? </p><p class="body-paragraph" data-auto="body_paragraph">A 
quick glance back over this chapter might leave the reader with the 
impression that only one approach to statistical inference (i.e., the 
inductive process of generalizing from a sample of observations to 
population quantities using a probabilistic argument) appeared during 
this period, namely, the inverse probability method we now associate 
with the name of Bayes. This is not quite accurate because, in 1778, 
Daniel Bernoulli published a memoir on his choice of error curves in 
which he proposed with little justification a method of estimation that 
we can now recognize as what Stigler refers to as "an adumbration of 
maximum likelihood." Bernoulli's method was harshly criticized as 
arbitrary by Euler in an appended commentary and was quickly 
overshadowed by Laplace's work. Thus, Bernoulli's contributions played 
no known role in the later rediscovery of maximum likelihood in the 
twentieth century. </p><a id="AN9709120309-6"> </a><span class="medium-bold"><h3><a data-auto="ep_link" href="#toc" onclick="FocusElement('toc');" id="hd_toc_AN9709120309-6" title=" CHAPTER 3: THE SOCIALIZATION OF STATISTICS AND THE DEVELOPMENT OF CORRELATION AND STATISTICAL MODELS: 1820-1900  "> CHAPTER 3: THE SOCIALIZATION OF STATISTICS AND THE DEVELOPMENT OF CORRELATION AND STATISTICAL MODELS: 1820-1900  </a></h3></span><p class="body-paragraph" data-auto="body_paragraph">As
 we open this chapter in 1824, we find the Belgian astronomer and 
mathematician Adolphe Quetelet in Paris reaming about probability and 
statistics from Joseph Fourier, who had in turn learned these ideas from
 Laplace. Quetelet returned to Brussels and immediately attempted to 
apply Laplace's 1780 method of ratio estimation using birth and death 
rates as part of the analysis of past census data and in planning for an
 1829 census. Although attracted by the power of this method, Quetelet 
finally backed away from its full-scale implementation because of the 
quantities that required measurement and the differences among groups in
 the population for which he needed to account. Henceforth, he 
concentrated on the macroanalysis of social data using statistical 
methods reamed from Laplace and others, but he eschewed the 
probabilistic analysis of uncertainty in estimates and measurements that
 was inherent in the Laplace method that he had earlier recommended. </p><p class="body-paragraph" data-auto="body_paragraph">Quetelet
 may well be considered the father of quantitative social science, and 
his two major contributions toward the statistical analysis of social 
data, the concept of "the average man" in his 1835 book and the later 
fitting of the normal distribution to several social science data sets 
coupled with the interpretation of the stability of the corresponding 
social phenomena, dominated the field for many years. Nonetheless, 
Stigler argues that, despite these and other contributions, Quetelet's 
attempt to import the methods of statistics to the realm of social 
science were failures because he resisted turning to the individual 
level and the study of relationships among variables in order to explain
 the heterogeneity that he found blocking the use of the known 
statistics methods of his day. </p><p class="body-paragraph" data-auto="body_paragraph">Porter,
 on the other hand, focuses on Quetelet's view of statistical social 
science as social physics and sees less failure and much greater 
influence, extending to the development of the kinetic theory of gases. 
He traces a path from the pioneering 1859 paper by the Scottish 
physicist James Clerk Maxwell to Quetelet through a review of one of his
 books by John Herschel and through Henry Thomas Buckle's History of 
Civilization in England, which presented an exaggerated account of 
Quetelet's findings of statistical regularities. Porter also finds 
analogies between probabilistic molecular behavior and "the statistical 
behavior of a free society" in the later work of Maxwell and Boltzmann. </p><p class="body-paragraph" data-auto="body_paragraph">But,
 in the words of Porter, it was during this period that we see the rise 
in statistical thinking, as others ventured forward along paths that 
Quetelet was unable to follow. Contributors to the development of 
statistical thinking during this period, especially those with an 
orientation toward its application in the social and behavioral 
sciences, include Poisson, I. J. Bienayme (Heyde and Seneta 1977), 
Wilhelm Lexis, William Farr, Auguste Comte (in a negative fashion), 
Antoine Augustin Cournot, and Gustov Theodor Fechner (followed somewhat 
later by Hermann Ebbinghaus). Daston singles out Cournot's work as 
marking "the advent of a new interpretation of mathematical probability 
exclusively in terms of objective frequencies. [He] was also among the 
first to recognize that the classical approach to probability had been 
an interpretation, distinct from mathematical probability per se" (p. 
224). Porter also suggests that it was with the rise of social 
statistics and the idea of statistical regularities that we see a shift 
from the subjective notion of probability inherent in the work of 
Laplace to the frequency notion, which was to emerge fully only in the 
twentieth century.(<a id="bib8up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib8" id="ref_linkbib8" title="n8">n8</a>) </p><p class="body-paragraph" data-auto="body_paragraph">Yet
 despite all the intellectual attention toward statistics, as the 1870s 
approached, there had not been a major breakthrough in the development 
of statistical methodology that could rank with the development of least
 squares and its link to the normal distribution and the central limit 
theorem. The period from 1880 to 1900 saw a notable change in the pace 
of statistical developments, especially in England, as a result of the 
contributions and leadership of Francis Galton, Francis Ysidro 
Edgeworth, Karl Pearson, and George Udny Yule. There was also the 
related work of others, such as the logician John Venn, but the lines of
 work carried out by this quartet of scientists seem to be the ones most
 critical to the development of a basic approach to statistical 
methodology. </p><p class="body-paragraph" data-auto="body_paragraph">Stigler
 describes Galton as a "romantic figure in the history of statistics, 
perhaps the last of the gentleman scientists" (p. 266). Before turning 
to statistics, he explored Africa and studied meteorological problems 
and heredity. In fact, it was through his study of Hereditary Genius 
(Gallon 1869) that we get the first glimmerings of Galton's own ideas on
 regression. Over the next sixteen years, these ideas sharpened and, in 
his 1885 presidential address to the anthropological section of the 
British Association for the Advancement of Science, Galton presented, in
 a table and diagram reproduced by Stigler, the first statistical 
description of the phenomenon of regression and its link to normal 
distributions. This was done in the context of an empirical example of 
the regression of children's heights on the average of their parents' 
heights. A few years later, in 1888, Galton formulated the related 
concept of correlation. Porter and Stigler present complementary, 
detailed descriptions of these developments. </p><p class="body-paragraph" data-auto="body_paragraph">Galton's
 ideas on regression and correlation were quickly picked up and extended
 by others. Most notable in this regard was the work of Edgeworth, who, 
during the 1880s, tried to take the ideas of statistical analysis that 
were developed decades earlier in astronomy and geodesy and apply them 
to social and economic statistics. Because of his knowledge of least 
squares and inverse probability, Edgeworth was able to link Galton's 
concepts of regression and correlation to these earlier methods, and he 
did so directly in the context of multivariate normal distributions for 
which he introduced the equivalent of modern notation for the 
correlation matrix. </p><p class="body-paragraph" data-auto="body_paragraph">Then
 came the contributions of Karl Pearson and Yule, which are described in
 the final chapter of Stigler's book. Stigler attempts to provide direct
 evidence of the influence of Edgeworth on the evolution of Pearson's 
ideas about statistical methods. In the early 1890s, Pearson began to 
write about methods for the analysis of skew curves, which led to the 
formulation of the Pearson family of curves. Pearson also lectured on 
the history of statistical methods during this period (see Pearson 
1978). During some of these lectures, Pearson introduced Yule to the 
study of skew curves and correlation. According to Stigler, Yule 
provided statistics with its second synthesis that reconciled the 
developments in the theory of correlation and regression with the 
earlier methodology of least squares and the theory of errors. In an 
1897 paper, Yule presented this synthesis and introduced the concepts of
 multiple and partial correlation. A decade later, he introduced the 
modern notation for regression analysis that is still widely used today.
 Then the synthesis was truly complete. </p><p class="body-paragraph" data-auto="body_paragraph">The
 nineteenth century ended with other new contributions by Pearson and 
his collaborators in the biometrical school, for example, his chi-square
 test for goodness of fit in contingency tables and his founding of the 
first independent methodologically oriented statistics journal, 
Biometrika, with Galton and W. F. R. Weldon (with funding from Galton). 
For Stigler, however, it is not the varied contributions of Pearson but 
the second synthesis of Yule that provides the watershed for the 
creation of the modern field of statistics: </p><p class="body-paragraph" data-auto="body_paragraph">The
 conceptual triumphs of the nineteenth century had been the product of 
many minds working on problems in many fields, and one of the most 
striking of their accomplishments was the creation of a new discipline. 
Before 1900 we see many scientists of different fields developing and 
using techniques we now recognize as belonging to modem statistics. 
After 1900 we beam to see identifiable statisticians developing such 
techniques into a unified logic of empirical science that goes far 
beyond its component parts. There was no sharp moment of birth; but with
 Pearson and Yule and the growing number of students in Pearson's 
laboratory, the infant discipline may be said to have arrived. And that 
infant was to find no shortage of challenges. (p. 361) </p><p class="body-paragraph" data-auto="body_paragraph">Moreover,
 as Ian Hacking (1990, 2) describes the situation, "By the end of the 
century chance had attained the respectability of a Victorian valet, 
ready to be the logical servant of the natural, biological and social 
sciences." </p><a id="AN9709120309-7"> </a><span class="medium-bold"><h3><a data-auto="ep_link" href="#toc" onclick="FocusElement('toc');" id="hd_toc_AN9709120309-7" title=" CHAPTER 3 1/2: 1900 1950  "> CHAPTER 3 1/2: 1900 1950  </a></h3></span><p class="body-paragraph" data-auto="body_paragraph">And
 so the rest is history, or so the saying goes. But is it? The field of 
statistics as we know it today emerged only in the twentieth century, 
building on the Gauss-Laplace synthesis described in chapter 2 and the 
treatment of regression by Galton, Edgeworth, Pearson, and Yule 
described in chapter 3. </p><p class="body-paragraph" data-auto="body_paragraph">It
 is only after the turn of the century that we find the rise of theories
 of statistical inference, using probabilistic notions in a systematic 
way to gather, analyze, and summarize scientific data. Chapter 3 of The 
Empire of Chance captures much of this development, telling part of the 
story of the English statistician Ronald A. Fisher (1890 1962) and his 
development of the notions of a statistical model, sufficiency, 
likelihood, the statistical concept of randomization, the theory of 
experimental design, and the method of the analysis of variance. To many
 statisticians, Fisher is the greatest statistician of the century and 
these contributions, most of which came in a relatively short time span 
in the 1920s, permanently altered the course of statistical 
development.(<a id="bib9up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib9" id="ref_linkbib9" title="n9">n9</a>)
 His work built on that of the nineteenth century and especially on 
papers by Edgeworth, Karl Pearson, and Yule, as well as those by William
 S. Gosset,(<a id="bib10up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib10" id="ref_linkbib10" title="n10">n10</a>)
 who worked at the Guinness Brewery in Dublin and interacted extensively
 with Pearson (see Pearson 1990). Fisher shared Pearson's enthusiasm for
 Charles Darwin's theory of the origin of species by natural selection, 
and he pursued a highly creative career in genetics as well as his 
career as a statistician. Although Pearson published Fisher's 1915 paper
 on the distribution of the sample correlation coefficient in 
Biometrika, the two men quickly had a falling out and engaged in public 
debates and acrimonious printed exchanges until Pearson's death. Gosset 
was friendly with both Fisher and Pearson and often played a mediating 
role in the clashes between these two statistical titans, although he 
too had his own disputes with Fisher.(<a id="bib11up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib11" id="ref_linkbib11" title="n11">n11</a>) </p><p class="body-paragraph" data-auto="body_paragraph">Fisher
 entered Cambridge in 1909 and published first paper while still an 
undergraduate in 1912. In 1919 he accepted the newly created position of
 statistician at the Rothamsted Experimental Station. It was while at 
Rothamsted that Fisher's early statistical ideas were shaped and 
focused. Fisher's statistical ideas had an immediate impact on the 
scientific analysis of data, for example, the design of experiments and 
the analysis of variance, but some of the concepts he introduced sparked
 considerable controversy. In particular, his formulation of tests of 
significance and his fiducial approach to probabilistically based 
interval estimation produced the greatest statistical controversy of the
 first half of the century. Fisher's fiducial method provided a method 
of inverting probability statements about observations given the values 
of parameters into probability statements about parameters given the 
observations without the use of Bayes's Theorem, which provides a 
mechanism for such an inversion. </p><p class="body-paragraph" data-auto="body_paragraph">Jerzy
 Neyman, a Polish statistician, came to England in 1925 to work in Karl 
Pearson's laboratory, where he struck up a collaboration with Pearson's 
son, Egon, that led to the theory of hypothesis testing, which they 
claimed improved on Fisher's significance testing approach by explicitly
 recognizing the role of rival hypotheses. In one of the most important 
papers of the century, Neyman (1934) laid the foundation for the 
statistical theory of sampling and gave the first description of the 
confidence method of interval estimation,(<a id="bib12up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib12" id="ref_linkbib12" title="n12">n12</a>)
 which is based on an infinite sequence of repeated samples. Then 
Neyman, both separately and in collaboration with Pearson, elaborated on
 the method of confidence intervals linking it to their theory of 
testing hypotheses. Although Fisher was relatively accepting of the 
early papers by Neyman and Pearson, ultimately, he argued vigorously 
against their embellishments.(<a id="bib13up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib13" id="ref_linkbib13" title="n13">n13</a>)
 His debate with Neyman and Pearson on issues involving testing and 
interval estimation, which began in the mid-1930s was replete with 
rhetorical flourishes. As Gigerenzer et al note in The Empire of Chance:
 </p><p class="body-paragraph" data-auto="body_paragraph">Fisher never 
perceived the emerging Neyman-Pearson theory as correcting and improving
 on his own work on tests of significance. Right up to his death in 1962
 he rejected the key concepts of the Neyman-Pearson theory, such as 
"errors of the second kind," "repeated sampling from finite 
populations," and "inductive behavior." His recurring reproach was that 
Neyman and Pearson were mere mathematicians without experience in the 
natural sciences, and that their work reflected this insulation from all
 living contact with real scientific problems. (p. 98) </p><p class="body-paragraph" data-auto="body_paragraph">Over
 time, the flaws of Fisher's fiducial method became apparent to many 
statisticians and, as is described m The Empire of Chance, a somewhat 
curious hybrid of his approach to tests of significance and the 
Neyman-Pearson theory quickly spread to various fields of application. 
In 1937, Neyman moved to the United States and helped stimulate the 
adaptation and development of his ideas on sampling to large-scale 
national sample surveys under government auspices. Neyman was a key 
figure in the rise of mathematical statistics in the United States, 
which occurred in the late 1930s and 1940s, along with Harold Hotelling,
 Abraham Wald, and Samuel Wilks. </p><p class="body-paragraph" data-auto="body_paragraph">A
 curious omission in Gigerenzer et al.'s description of "the inference 
experts" is one of the emergence of the Bayesian or subjectivist school 
of statistical inference, which occurred at approximately the same time 
as the development of the Neyman-Pearson theory. In the mid-1920s, Frank
 Ramsay, reacting to the ideas of others, such as John Maynard Keynes, 
set out an approach to probability based on personal degrees of belief 
and linked these to notion of utility, drawing on the original 
formulation of Daniel Bernoulli in 1738. This subjective perspective was
 independently justified by Bruno de Finetti (1930, 1937) in terms of 
coherence or consistency and ultimately synthesized with further 
technical work on utility by L. J. Savage (1954) decades later. Coupled 
with Harold Jeffreys's (1939) systematic treatment of the use of 
Bayesian methods in statistical inference (albeit from a somewhat 
objective perspective that is decidedly not decision-theoretic), these 
authors laid the foundation for the Bayesian revival that has occured 
over the past three decades. </p><p class="body-paragraph" data-auto="body_paragraph">In
 passing, Gigerenzer et al. also duly note the key contribution to 
probability theory in the early 1930s by the Russian mathematician A. N.
 Kolmogorov, who laid down an axiomatic approach to the theory of 
probability, drawing on the mathematical fields of set theory and the 
theory of functions, which launched probability as a separately 
identifiable subfield of mathematics. The Kolmogorov axioms and 
set-theoretic approach provided a mathematical foundation to the earlier
 classical theory of probability and provided a powerful basis for the 
proof of mathematical results in statistical theory. </p><p class="body-paragraph" data-auto="body_paragraph">Gigerenzer
 et al. end their description of "the inference experts" with two 
sections on the statistics profession, which describe the specialization
 of statistical knowledge and its institutionalization, especially in 
the form of statistical laboratories and departments of statistics in 
universities. The earliest of these was Karl Pearson's Biometric 
Laboratory at University College, London, which dates back to 1895.(<a id="bib14up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib14" id="ref_linkbib14" title="n14">n14</a>)
 It was followed, in 1932, by the Statistical Laboratory at Iowa State 
College founded by George Snedecor and the creation of a large number of
 departments that were separate from mathematics across the United 
States over the next thirty-five years. The special role of statistics 
in aiding the allied effort in World War II served as a stimulus in both
 the United States and Great Britain to the development and expansion of
 statistics as a field and to the creation of separate departments in 
universities. This is duly noted in The Empire of Chance. </p><p class="body-paragraph" data-auto="body_paragraph">This
 brings to an end the developments in the history of probability and 
statistics as they are chronicled in the volumes under review. Although 
Gigerenzer et al. do describe subsequent applications in various 
domains, they do not do so in any systematic fashion nor do they link 
these developments to the further development and specialization of 
statistical knowledge. Thus, I have chosen to end this review somewhat 
short of the midpoint of the current century. </p><a id="AN9709120309-8"> </a><span class="medium-bold"><h3><a data-auto="ep_link" href="#toc" onclick="FocusElement('toc');" id="hd_toc_AN9709120309-8" title=" EPILOGUE  "> EPILOGUE  </a></h3></span><p class="body-paragraph" data-auto="body_paragraph">Two
 of the volumes under review are titled The Probabilistic Revolution 
and, in volumes I and 2, the various contributing authors attempt to 
argue that during the nineteenth century there was a scientific 
revolution that produced a major paradigm shift associated with the 
adoption of probabilistic thinking. Indeed, Kruger, Daston, and 
Heidelberger begin with a chapter by Thomas Kuhn who summarizes ideas 
from his classic 1962 book (see Kuhn 1970), which laid out the 
distinction between the normal evolutionary mode of scientific progress 
and revolutionary change. I found the arguments in support of the 
occurrence of such a probability revolution to be the weakest feature of
 these books. Part of the problem comes from the focus that many of the 
authors have on probability rather than on statistics (which focuses on 
inferential issues that link probability to actual data). My 
interpretation of nineteenth-century history is that the spread of 
probabilistic ideas into several areas of science was evolutionary in 
nature. The basic mathematical structure of probability was already 
widely accepted and the adoption of probabilistic models for phenomena 
was a natural extension to deterministic approaches. </p><p class="body-paragraph" data-auto="body_paragraph">Gigerenzer
 et al. also talk about the probabilistic revolution, specifically in 
the field of physics, focusing on the change in interpretation of 
physical phenomena that occurred in 1860 when Maxwell used the law of 
error to describe the velocities of gases. Their argument, which ties in
 to one of the foci in Porter's book, is a bit more convincing but is 
restricted to the field of physics. Even in physics, the triumph of 
probabilistic thinking over determinism is far from complete as one can 
note from the current fascination of physical scientists with chaos 
theory and other mathematical devices that seemingly explain what others
 describe as stochastic behavior. </p><p class="body-paragraph" data-auto="body_paragraph">For
 me, if there was a scientific revolution during this period, it was 
really a result of the statistical ideas associated with what Stigler 
calls the Gauss-Laplace synthesis, which combined the normal error 
theory with the curve-fitting method of least squares into an 
inferential approach to the analysis of data using linear models. Yet, 
it took another 75 to 100 years and Galton's and Yule's formulation of 
regression before these ideas were used far beyond the boundaries of the
 astronomical problems that were addressed by Laplace and Gauss. Such 
delays are consistent with Kuhn's notions because the adoption of a 
paradigm shift often requires an entirely new generation of scientists 
who are open to different ways of scientific thinking. </p><p class="body-paragraph" data-auto="body_paragraph">I
 actually have a second candidate to propose as the focus for a 
revolution in statistical thinking: the 1920s and 1930s contributions of
 Fisher, Neyman, and Pearson for designing and making inference from 
randomized experiments and randomly selected samples. The truly novel 
component to their statistical ideas is the injection of a probabilistic
 component into a scientific problem through the design introduced by 
the scientist and the use of this component to make inferences about 
relevant hypothesis or population quantities. Following the application 
of the Fisher-Neyman-Pearson ideas to selected problems in the late 
1930s and World War II, we have seen what is close to the universal 
adoption of their approach to virtually all areas of science. The 
possible exceptions to this spread are the "great observational 
sciences" of astronomy and physics that were so important to Laplace and
 Gauss. </p><p class="body-paragraph" data-auto="body_paragraph">In the 
traditional approach to statistics that emerged from the Gauss-Laplace 
synthesis and was developed by the statisticians at the turn of the 
century, probability enters a scientific problem as a property of the 
state of nature. The key new feature of the Fisher approach to 
experimentation and the parallel Neyman approach to experimentation and 
the parallel Neyman approach to sampling is the introduction of 
probability to the problem through a probability-based randomization 
mechanism, such as a table of "random numbers." This externally 
introduced probability is not part of the state of nature, but it is 
used to make inferences from the experiment or sample to a population. 
Indeed, the methodology does not depend on whether nature is totally 
deterministic or at least partially stochastic. The statistical methods 
that have emerged over the subsequent fifty to sixty years represent an 
amalgam of the Fisher-Neyman randomization ideas and the ideas 
concerning the stochastic features of nature, with some statisticians 
relying solely on randomization for inferences, others on statistical 
models with stochastic components, and still others on some mixture of 
the two. </p><p class="body-paragraph" data-auto="body_paragraph">To 
write a truly comprehensive history of only a selective period of an 
area of science often requires a heroic effort that goes unrecognized by
 non-historians. Such an effort typically involves the painstaking 
examination of source materials, which, in the case of Stigler, included
 the examination of the marginal notes of one statistician in his 
personal copy of an earlier work. It also requires attempting to resolve
 competing claims of priority for various discoveries in circumstances 
where authors were not necessarily familiar with or at least did not 
comprehend the contribution of earlier related results of others. It 
requires the examination of secondary and even tertiary sources in order
 to reassess the influence and importance of diverse primary 
contributions. A future difficulty arises as one surveys the landscape 
of intellectual activities in a given period. The greater the relevant 
literature, the more difficult the dual task of being comprehensive 
while describing the "big picture." </p><p class="body-paragraph" data-auto="body_paragraph">My
 overview in this review essay is garnered in large part from the books 
under review and involves only limited examination of primary sources. 
Where my familiarity with the primary sources is greatest, namely in the
 twentieth century, I take the greatest issue with the authors trying to
 describe this period. But, since many of the accomplishments of the 
twentieth century may still be too fresh for us to sort out, this 
disagreement might have well occurred even if I was far less familiar 
with the materials. </p><p class="body-paragraph" data-auto="body_paragraph">Each
 major author and editor in the septet of books under review succeeds in
 giving a reasonably good view of his or her selected topics and periods
 from the history of statistics. While there are often quite different 
perspectives and commentaries on the relative importance of specific 
authors and specific works, this variety is what I would expect from 
authors with such diverse backgrounds and interests. When the volumes 
are viewed collectively, their coverage of statistical topics is almost 
complete. At first, one might express surprise at the total absence of 
references to William Playfair, whose compelling statistical graphics 
have been the focus of considerable recent attention. But Playfair 
worked without the use of probabilistic ideas in the school of political
 arithmetic that descended from Graunt, and thus his work does not play a
 key role in the development of statistics during the nineteenth century
 following the publication of his books. </p><p class="body-paragraph" data-auto="body_paragraph">The
 one topic I found to be "neglected" is the development of sample 
surveys and the related statistical methodology as a formal statistical 
enterprise. There are some related reference sin Porter's and Stigler's 
books (especially as census taking provides part of the foundation of 
survey taking) and a somewhat obscure footnote in Daston's book (p. 
360), but these discussions are brief at best.(<a id="bib15up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib15" id="ref_linkbib15" title="n15">n15</a>)
 Moreover, Gigerenzer et al., who focus on twentieth-century 
contributions, devote a scant two to three pages (referring primarily to
 secondary sources) in order to describe what I view as one of the more 
remarkable achievements in statistics over the past century.(<a id="bib16up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib16" id="ref_linkbib16" title="n16">n16</a>)
 But this is a minor flaw in such an important set of books that 
otherwise gives the reader an excellent sense of the intellectual 
origins of statistical thinking and the way that statistical ideas were 
created and developed. </p><p class="body-paragraph" data-auto="body_paragraph">Hald's
 book provides an interesting compendium of material on the prehistory 
of statistics. Some of the chapters provide real insight into the 
detailed discussion in source materials and into the interrelationships 
among various contributions. It also includes some seemingly irrelevant 
digressions, for example, on mathematics and natural philosophy before 
1650 and on the Newtonian revolution in mathematics and science,(<a id="bib17up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib17" id="ref_linkbib17" title="n17">n17</a>) as well as a confusing discussion about Johannes Kepler and data analysis,(<a id="bib18up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib18" id="ref_linkbib18" title="n18">n18</a>)
 which is unlinked to any other material in the book. But this is a 
minor quibble. Of far greater concern to me is the disjointed nature of 
the presentation, with material from a single source being presented in 
seemingly unrelated sections and chapters. A great part of the value of 
Hald's book for statistics is his focus on the details of the technical 
arguments, including the reproduction of proofs, updated somewhat so 
that the notation conforms more closely to the modern usage. How much 
interest this will hold for the historian of science I am not sure. For 
example, by the time Hald gets to De Moivre's recursion formula for the 
"duration of play" in a two-person probabilistic game, we are awash in 
combinatoric formulae and equations. Even for the statistician, this 
suddenly reads more like a text on combinatorial probability than like 
one on the history of probability and statistics. </p><p class="body-paragraph" data-auto="body_paragraph">Daston's
 book stands in sharp contrast to Hald's. It contains few technical 
arguments and tends to stress crosscutting historical and philosophical 
themes. Indeed her book is organized by these themes, and, instead of 
proceeding in a chronological fashion, it sweeps back and forth across 
the terrain of over two hundred years of writing on probability, 
covering themes such as expectation, the theory and practice of risk, 
the meaning of probability, the probability of causes, and moralizing 
mathematics. Her prose is carefully crafted and the text contains many 
insights, but I am afraid that many of them would have been lost on me 
had I not read Hald and Stigler first. Ruth Schwartz Cowan (1987), (<a id="bib19up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib19" id="ref_linkbib19" title="n19">n19</a>)
 in a review of Stigler's and Porter's books, notes the difference 
between two approaches to the writing of the history of science: 
externalism and internalism. For the prehistorical period, Daston is the
 externalist, always looking to the larger intellectual issues, and Hald
 is the internalist forever wallowing in the details of formulas and 
proofs. </p><p class="body-paragraph" data-auto="body_paragraph">Stigler's
 book is meticulous in detail, as he attempts to adjudicate among 
conflicting claims over interpretation of early works (e.g., of Bayes 
and Laplace), to speculate explicitly about what some authors meant, and
 to try to develop a consistent interpretation of seemingly conflicting 
historical statements or claims of proof. He brings a modern statistical
 outlook to otherwise archaic technical arguments but always takes care 
to explain what we can infer about what was known at the time the author
 in question was writing. But most importantly, Stigler writes, not in a
 dry and boring fashion that some think appropriate for history, but in a
 way that draws the reader in with the fascination of solving a mystery 
of paramount importance. And everywhere, there is a touch of wit as in 
the remark about the Bernoullis: </p><p class="body-paragraph" data-auto="body_paragraph">The
 Bernoullis are surely the most renowned family in the history of the 
mathematical sciences. Perhaps as many as twelve Bernoullis have 
contributed to some branch of mathematics or physics, and at least five 
have written on probability. So large is the set of Bernoullis that 
chance alone may have made it inevitable that a Bernoulli should be 
designated father of the quantification of uncertainty. (p. 63) </p><p class="body-paragraph" data-auto="body_paragraph">Porter's
 book has few technical arguments and mathematical formulas, but his 
prose is not light. He sets the story of chapter 3 of our history of 
statistics much more firmly on its social roots and leans more on social
 explanations and liberal politics than on statistical and mathematical 
developments. As such, it is a much easier "read" than Stigler's books 
or the related chapters of volumes 1 and 2 of The Probabilistic 
Revolution. The contrast between Stigler and Porter is best seen in 
their discussions of Quetelet: Porter focuses on Quetelet's work on 
social statistics as social physics, while Stigler finds examples of 
early contributions to such topics as the analysis of variance and 
normal probability plots; Porter sees Quetelet as "lacking . . . the 
genius to formulate a usable mathematical procedure for analyzing 
statistical information," whereas Stigler finds ingenious methods of 
calculation. In her review of Stigler's and Porter's books, Cowan notes 
that they have written markedly different accounts, separated largely by
 the distinction between externalism and internalism. Stigler, the 
statistician and thus the internalist, focuses on the scientific texts, 
whereas Porter, the historian and externalist, searches more to learn 
what led key individuals to work on the problems that they chose and in 
the manner that they did. In my readings of the two books, I found 
Porter more prone to generalization than Stigler and far less 
knowledgeable about the importance of the statistical ideas under 
discussion. Nonetheless, his historical perspective provides a valuable 
supplement to Stigler's insights and observations on the technical 
details. </p><p class="body-paragraph" data-auto="body_paragraph">The 
two volumes of The Probabilistic Revolution provide some useful 
supplements to the material in Stigler and Porter, but their primary 
focus on probability rather than statistics leads them into lacunae that
 are diverting at best from the broad picture of the historical 
developments of statistics. With the exception of the brief chapter by 
Stigler in volume I of The Probabilistic Revolution, which closely 
resembles the introduction to his book but with some subtle statistical 
differences, the chapter authors are not statisticians and their 
externalist perspectives are not in line with my own. </p><p class="body-paragraph" data-auto="body_paragraph">The
 septet of books described in this review form a major addition to and a
 revamped perspective on the history of statistics. As fields, both 
statistics and the history of science are fortunate to have these 
contributions that fill in many of the gaps in our previous 
understanding of the evolution of statistics as a field. The books 
belong in every library and on the shelves of those who take the study 
of the development of scientific concepts and methods seriously. </p><p class="body-paragraph" data-auto="body_paragraph">Many
 readers will not have the time or patience to pore over the thousands 
of pages represented by these volumes. For them, I recommend Stigler's 
book for its coverage of the two most important chapters of the history 
described here and a good piece of the prehistory as well, for its 
scholarship and balance between technical and contextual materials, and 
for the elegance of its prose. And in its newly issued paperback 
edition, Stigler's book is one that belongs on the shelf of every 
statistician and historian of probability and statistics. After Stigler,
 I suggest a selection of material from Porter's book, especially the 
intriguing sections on the influence of Quetelet on Maxwell and 
Boltzmann. If the reader's interest is in the twentieth-century 
developments and the controversies involving Fisher and Neyman, the 
reader might choose toe go to their biographies.(<a id="bib20up
					"> </a><a data-auto="ep_link" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib20" id="ref_linkbib20" title="n20">n20</a>) </p><a id="AN9709120309-9"> </a><span class="medium-bold"><a data-auto="ep_link" href="#toc" id="ref_toc" title="NOTES">NOTES</a></span><p class="body-paragraph"><em><a id="bib2"> </a><a data-auto="ep_link_bt" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib2up" id="bib_upbib2" title="(n2.)">(n2.)</a>
 As Stigler notes in The Rise in Statistical Thinking. 1820 1900, 
"Citation was an imperfect an in the eighteenth century" (p. 95).  </em></p><p class="body-paragraph"><em><a id="bib3"> </a><a data-auto="ep_link_bt" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib3up" id="bib_upbib3" title="(n3.)">(n3.)</a>
 A Brief History of Time by Stephen W. Hawking (1988) and The History of
 the World in 10% Chapters by Julian Barnes (1989). Emulating the first 
author, I would like to expose non-statisticians to the intellectual 
excitement associated with the emergence of reasoning with uncertainty. 
Unlike the second author, I place my half chapter at the end, for 
reasons that I will soon explain.  </em></p><p class="body-paragraph"><em><a id="bib4"> </a><a data-auto="ep_link_bt" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib4up" id="bib_upbib4" title="(n4.)">(n4.)</a>
 Ruth Schwartz Cowan (1987) discusses some of these issues in her review
 of Porter and Stigler, and I attempt to explain her views in the 
epilogue.  </em></p><p class="body-paragraph"><em><a id="bib5"> </a><a data-auto="ep_link_bt" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib5up" id="bib_upbib5" title="(n5.)">(n5.)</a>
 During the preparation of a forthcoming pair of volumes of 
Breakthroughs in Statistics, Norman Johnson and Samuel Kotz polled a 
group of prominent statisticians to determine which contributions to 
include. They found considerable agreement on which papers and books 
published prior to 1950 constituted breakthroughs and quite divergent 
views on those that appeared in the 1960s and 1970s (Johnson and Kotz, 
forthcoming).  </em></p><p class="body-paragraph"><em><a id="bib6"> </a><a data-auto="ep_link_bt" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib6up" id="bib_upbib6" title="(n6.)">(n6.)</a>
 Hacking (1975) asks why the time was ripe for the emergence of our 
current concept of probability at about 1660, and he reviews many of the
 contributions in the ensuing half century.  </em></p><p class="body-paragraph"><em><a id="bib7"> </a><a data-auto="ep_link_bt" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib7up" id="bib_upbib7" title="(n7.)">(n7.)</a>
 Daston (chapter 6) describes how various authors pursue the theme 
linking statistical arguments to moral issues over the next two hundred 
years. The ideas on utility of Daniel Bernoulli ultimately had a major 
impact on the work by Frank Ramsay in the 1920s and serve as a direct 
underpinning for modem statistical decision theory. See Savage (1954) as
 well as the beef discussion of this topic in chapter 3 1/2 below.  </em></p><p class="body-paragraph"><em><a id="bib8"> </a><a data-auto="ep_link_bt" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib8up" id="bib_upbib8" title="(n8.)">(n8.)</a>
 Daston's chapter in volume I of The Probabilistic Revolution (chapter 
13, pp. 295-304) provides support for this view while the essay by 
Kamlah (chapter 5, pp. 91-116) offers a rather different explanation of 
the shift to the frequency interpretation of probability, which totally 
ignores all of the nineteenth-century contributions to statistics (as 
distinct from contributions to probability theory viewed as a branch of 
mathematics).  </em></p><p class="body-paragraph"><em><a id="bib9"> </a><a data-auto="ep_link_bt" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib9up" id="bib_upbib9" title="(n9.)">(n9.)</a>
 For evaluations of the contributions of Fisher, see George A. Barnard 
(1990), Fienberg and David V. Hinkley (1980), and L. J. Savage (1976). 
Fisher's collected works, consisting of 140 papers on genetics, 129 on 
statistics, and 16 on other topics (in addition to various reviews) have
 been published in a six-volume set and his four books have continued in
 print. On the occasion of the 100th anniversary of his birth, Oxford 
University Press reissued his three statistics books in a special single
 volume.  </em></p><p class="body-paragraph"><em><a id="bib10"> </a><a data-auto="ep_link_bt" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib10up" id="bib_upbib10" title="(n10.)">(n10.)</a>
 Because of restrictions on publication by Guinness, Gosset published 
under the pseudonym "Student," a name still associate with the 
t-distribution, which he invented.  </em></p><p class="body-paragraph"><em><a id="bib11"> </a><a data-auto="ep_link_bt" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib11up" id="bib_upbib11" title="(n11.)">(n11.)</a>
 For a discussion of the principal dispute between Fisher Gosset, see 
the paper by Picard in Fienberg and Hinkley (I and Egon Pearson's 
biography of Gosset (Pearson 1990).  </em></p><p class="body-paragraph"><em><a id="bib12"> </a><a data-auto="ep_link_bt" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib12up" id="bib_upbib12" title="(n12.)">(n12.)</a>
 Confidence intervals have probability-like properties that based on the
 long-run frequency of coverage of a hypothetical finite sequence of 
intervals, and thus they differ markedly in conception from Fisher's 
fiducial intervals and posterior probability intervals calculated from 
an application of Bayes's Theorem Hacking (1990) and others have argued 
that E. B. Wilson gave rationale for confidence interval statements 
prior to Neyman that Wilson's work had roots in an even earlier 
description due C. S. Pierce in the nineteenth century. The common 
attribution the ideas to Neyman may be another instance of a variant on 
Stigler's Law (see note 2 above).  </em></p><p class="body-paragraph"><em><a id="bib13"> </a><a data-auto="ep_link_bt" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib13up" id="bib_upbib13" title="(n13.)">(n13.)</a>
 Fisher did not object to the early papers by Neyman and pears and he 
prepared a positive referee's report on their 1933 that appeared in the 
Philosophical Transactions of the Royal society of London (see Reid 
1982. 103-104). He also made complimentary comments on Neyman's 1934 
paper when it was sensed at a meeting of the Royal Statistical Society. 
The dispute between Fisher and Neyman and Pearson seemed to develop of a
 critical exchange over a central point in a 1935 paper Neyman in the 
Journal of the Royal Statistical Society on statistical problems in 
agricultural experimentation. The rhetorical aspects of this dispute 
were then carried over to the basic issues inference on which they 
differed.  </em></p><p class="body-paragraph"><em><a id="bib14"> </a><a data-auto="ep_link_bt" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib14up" id="bib_upbib14" title="(n14.)">(n14.)</a>
 Gigerenzer et al. give the date as 1904. That was the year when Galton 
established the Eugenics Record Office. Two years let became known as 
the Eugenics Laboratory when Galton turn over to Pearson to operate. The
 two operations were officially merged in 1911 to form the Department of
 Applied Statistics Pearson as its first professor (Walker 1978).  </em></p><p class="body-paragraph"><em><a id="bib15"> </a><a data-auto="ep_link_bt" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib15up" id="bib_upbib15" title="(n15.)">(n15.)</a>
 At least Stigler does mention in passing Laplace's development ratio 
estimation using inverse probability and Quetelet's later proposal for 
implementation of it. But neither he nor Porter explains why it took so 
long for random selection of sampling units to introduced. There was a 
clear change in the mode of inference that accompanied the breakthrough 
in methodology for sample surveys which was tied to Neyman's 
pathbreaking 1934 paper the topic. These developments go well beyond the
 1900 boundary of the Porter and Stigler books.  </em></p><p class="body-paragraph"><em><a id="bib16"> </a><a data-auto="ep_link_bt" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib16up" id="bib_upbib16" title="(n16.)">(n16.)</a>
 For a brief overview of how the development of sample survey can be 
linked to other developments in statistics in the nines and twentieth 
centuries, see Fienberg and Judith M. Tanur (1990).  </em></p><p class="body-paragraph"><em><a id="bib17"> </a><a data-auto="ep_link_bt" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib17up" id="bib_upbib17" title="(n17.)">(n17.)</a>
 In such a major digression, Hald emulates Karl Pearson, who eluded 
major chunks of material in his lectures on topics such the 
Newton-Leibnitz controversy, which has little direct bearing on the 
history of probability and statistics. Most of the mate on such topics 
in Hald's book, however, are based on secondary or tertiary sources and 
thus offer few if any new insights. He fails to include Newton's 
connection to the Trial of the Pyx scribed in Stigler (1977).  </em></p><p class="body-paragraph"><em><a id="bib18"> </a><a data-auto="ep_link_bt" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib18up" id="bib_upbib18" title="(n18.)">(n18.)</a>
 Unlike the description suggested in Hald, Kepler did not that an 
ellipse gave a better fit than an ovoid to the data 0 planetary orbit of
 Mars. Instead, his choice, fortuitous thou was, was based on a somewhat
 arbitrary evaluation and the that the ellipse was more mathematically 
tractable than the ovoid. For a related discussion, see Fienberg (1985).
 In fact, recent evidence that appeared simultaneously with the 
publication Hald's book strongly suggests that the new data present 
Kepler in his 1609 book to support his elliptical orbit theory 
fabricated. See Donahue (1988).  </em></p><p class="body-paragraph"><em><a id="bib19"> </a><a data-auto="ep_link_bt" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib19up" id="bib_upbib19" title="(n19.)">(n19.)</a>
 In exploring the internalist/externalist distinction, Cowan criticizes 
Stigler for being so focused on the intellectual history of statistics 
that he fails to note the social origins of Galton's work correlation 
and his founding of the eugenices movement without the fine-grained 
intellectual history, all the contextual formation has, at best, limited
 value.  </em></p><p class="body-paragraph"><em><a id="bib20"> </a><a data-auto="ep_link_bt" href="https://web.b.ebscohost.com/ehost/delivery?sid=dc44e27c-921a-49db-8a7f-54d11c0e1489%40pdc-v-sessmgr06&amp;vid=26&amp;ReturnUrl=https%3a%2f%2fweb.b.ebscohost.com%2fehost%2fcommand%2fdetail%3fvid%3d25%26sid%3ddc44e27c-921a-49db-8a7f-54d11c0e1489%2540pdc-v-sessmgr06%26bdata%3dJmxhbmc9ZXMmc2l0ZT1laG9zdC1saXZlJnNjb3BlPXNpdGU%253d#bib20up" id="bib_upbib20" title="(n20.)">(n20.)</a> On Fisher by his daughter, Joan Box (1978), and on Neyman Constance Reid (1982).  </em></p><a id="AN9709120309-10"> </a><span class="medium-bold"><h3><a data-auto="ep_link" href="#toc" onclick="FocusElement('toc');" id="hd_toc_AN9709120309-10" title="REFERENCES">REFERENCES</a></h3></span><p class="body-paragraph" data-auto="body_paragraph">Barnard. G. A. 1990. Fisher: A retrospective. Chance 3(1): 22-28. </p><p class="body-paragraph" data-auto="body_paragraph">Barnes, J. 1989. The history of the world in 10% chapters. New York: Knopf. </p><p class="body-paragraph" data-auto="body_paragraph">Bernoulli, J. 1713. Ars Conjectandi. Basel: Thurnisiorum. </p><p class="body-paragraph" data-auto="body_paragraph">Box. J. F. 1978. R. A. Fisher, the life of a scientist. New York: Wiley. </p><p class="body-paragraph" data-auto="body_paragraph">Cowan, R. S. 1987. Book review of Porter and Stigler. Journal of the American Statistical Association 82: 1178-79. </p><p class="body-paragraph" data-auto="body_paragraph">de
 Finetti, B. 1930. Fondamenti logici del ragionamento probabilistico. 
Unione Matematica Italiana, Bolletino Series A 9: 258-61. </p><p class="body-paragraph" data-auto="body_paragraph">--.
 1937. La prevision: Ses lois logiques, ses sources subjectives. Annales
 de l'institut Henri Poincare 7, 1-68. Translated and reprinted in 
Studies in subjective probability, edited by H. E. Kyberg, Jr., and H. 
E. Smokler, 1964, 93-158. New York: Wiley. </p><p class="body-paragraph" data-auto="body_paragraph">De
 Moivre, A. 1738. The doctrine of chances: Or, a method of calculating 
the probability of events of play. 2d. ed. London: Woodfall. </p><p class="body-paragraph" data-auto="body_paragraph">Donahue,
 W. H. 1988. Kepler's fabricated figures: Covering up the mess in the 
new astronomy. Journal for the History of Astronomy 19(59): 217-37. </p><p class="body-paragraph" data-auto="body_paragraph">Fienberg,
 S. E. 1985. Statistics and the scientific method: Comments on and 
reactions to Freedman. In Cohort analysis in social research: Beyond the
 identification problem, edited by Wm. M. Mason and S. E. Fienberg, 
371-83. New York: Springer-Verlag. </p><p class="body-paragraph" data-auto="body_paragraph">Fienberg,
 S. E., and D. V. Hinkley. 1980. R. A. Fisher: An appreciation. Lecture 
Notes in Statistics, vol. 1. New York: Springer-Verlag. </p><p class="body-paragraph" data-auto="body_paragraph">Fienberg,
 S. E., and J. M. Tanur. 1990. A historical perspective on the 
institutional bases for survey research in the United States. Survey 
Methodology 16: 31-46. </p><p class="body-paragraph" data-auto="body_paragraph">Galton, F. 1869. Hereditary genius: An inquiry into its laws and consequences 1892. 2d. Ed. London: Macmillan. </p><p class="body-paragraph" data-auto="body_paragraph">Graunt, J. 1662. Natural and political observations made upon the bills of mortality. London: Martyn. </p><p class="body-paragraph" data-auto="body_paragraph">Hacking, I. 1975. The emergence of probability. Cambridge: Cambridge University Press. </p><p class="body-paragraph" data-auto="body_paragraph">--. 1990. The taming of chance. Cambridge: Cambridge University Press. </p><p class="body-paragraph" data-auto="body_paragraph">Hawking, S. W. 1988. A brief history of time. New York: Bantam. </p><p class="body-paragraph" data-auto="body_paragraph">Heyde, C. C., and E. Seneta. 1977.1. J. Bienayme: Statistical theory anticipated. Berlin: Springer-Verlag. </p><p class="body-paragraph" data-auto="body_paragraph">Jeffreys, H. 1939. Theory of probability. Clarendon: Oxford University Press. </p><p class="body-paragraph" data-auto="body_paragraph">Johnson, N. L., and S. Kotz. 1991. Breakthroughs in statistics 1890 1989. New York: Springer-Verlag. Forthcoming. </p><p class="body-paragraph" data-auto="body_paragraph">Kendall, M. G. 1960. Where shall the history of statistics begin? Biometrika 47: 447-49. </p><p class="body-paragraph" data-auto="body_paragraph">Kuhn, T. S. 1970. The structure of scientific c revolutions. 2d. ea., enlarged. Chicago: University of Chicago Press. </p><p class="body-paragraph" data-auto="body_paragraph">Neyman,
 J. 1934. On the two different aspects of the representative method: The
 method of stratified sampling and the method of purposive selection. 
(With discussion.) Journal of the Royal Statistical Society 97: 558-625.
 </p><p class="body-paragraph" data-auto="body_paragraph">Pearson, E. S.
 1990. "Student": A statistical biography of William Sealy Gosset, 
edited by R. L. Plackett and G. A. Barnard. New York: Oxford University 
Press. </p><p class="body-paragraph" data-auto="body_paragraph">Pearson,
 K. 1978. The history of statistics in the seventeenth and eighteenth 
centuries, against the changing background of intellectual, scientific, 
c, and religious thought, edited by E. S. Pearson. London: Charles 
Griffin. </p><p class="body-paragraph" data-auto="body_paragraph">Ramsay,
 F. P. [1926] 1964. Truth and probability. Reprint, in Studies in 
subjective probability, edited by H. E. Kyberg, Jr., and H. E. Smokler, 
61-92. New York: Wiley. </p><p class="body-paragraph" data-auto="body_paragraph">Reid, C. 1982. Neyman from life. New York: Springer-Verlag. </p><p class="body-paragraph" data-auto="body_paragraph">Savage, L. J. 1954. The foundations of statistics. New York: Wiley. </p><p class="body-paragraph" data-auto="body_paragraph">--. 1976. On rereading Fisher. Annals of Statistics 4: 441-500. </p><p class="body-paragraph" data-auto="body_paragraph">Stigler,
 S. M. 1977. Eight centuries of sampling inspection: The trial of the 
Pyx. Journal of the American Statistical Association 72: 493-500. </p><p class="body-paragraph" data-auto="body_paragraph">--. 1980. Stigler's law of eponymy. Transactions of the New York Academy of Science 2d series, 39: 147-57. </p><p class="body-paragraph" data-auto="body_paragraph">Todhunter,
 1. 1865. A history of the mathematical theory of probability. London: 
Macmillan. Reprinted, 1949 and 1965. New York: Chelsea. </p><p class="body-paragraph" data-auto="body_paragraph">Walker,
 H. M. 1978. Pearson, K. International encyclopedia of statistics, vol. 
2, edited by W. H. Kruskal and J. M. Tanur, 691-98. New York: Free 
Press. </p><p>~~~~~~~~</p><p class="body-paragraph" data-auto="body_paragraph">By Stephen E. Fienberg, College of Humanities and Social Sciences Carnegie-Mellon University </p><p class="body-paragraph" data-auto="body_paragraph"></p><p class="body-paragraph" data-auto="body_paragraph">Stephen
 E. Fienberg is Maurice Falk Professor of Statistics and Social Science 
and dean of the College of Humanities and Social Sciences at Carnegie 
Mellon University. This review essay was prepared in large part at the 
Hebrew University of Jerusalem, where he was Berman Visiting Professor 
during the winter of 1989-1990. I am indebted to several colleagues for 
ideas, critical comments, and editing on earlier drafts of this essay: 
Myron Gutmann, John Modell, Teddy Seidenfeld, S. James Press, Judith 
Tanur, and Joel Tarr. </p><div><hr width="25%" noshade="noshade"><span class="medium-normal">Copyright
 of Historical Methods is the property of Taylor &amp; Francis Ltd and 
its content may not be copied or emailed to multiple sites or posted to a
 listserv without the copyright holder's express written permission. 
However, users may print, download, or email articles for individual 
use.</span></div></div><pre class="preformatted-normal"></pre><br>
	</span></div>
	<div class="backButton">
		<a id="ctl00_MainContentArea_deliveryPrintSaveControl_backButtonBottom_lnkBack" title="Atrás" class="arrow-link prev delivmngr-toolbar-link" href="javascript:__doPostBack('ctl00$MainContentArea$deliveryPrintSaveControl$backButtonBottom$lnkBack','')">Atrás</a>
	</div>
</div>

			
			
			
			
			
		    
		</form>
	


</body></html>